{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FineCatIss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "- Implement exporting finetune model so it can be re-used for future runs iff export_finetuned_model is true\n",
    "- Implement adding PREDICTED_LABEL key to dictionary (json) if it is missing\n",
    "- Make os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, REPO_DATA_CONFIG_FILE_NAME) default config file for repo data config to avoid this output: No config file given. Trying to load config from \"/home/jovyan/config/repo_data_config.toml\"\n",
    "- Add different data filtering options train, eval and test data - currently issues always are assigned to the test set unless \"predicted_label\" is not None (OPTIONAL, low prio)\n",
    "- Option to ignore issues with predicted labels for fine-tuning as this could lead to worse performance\n",
    "  - not that important as issues with predictions have predicted_label not Null and therefore will get filtered out... this would only matter if the same labels as usual are used for predicted labels and then data being rebuild from scratch so that predicted_label always is Null again\n",
    "- implement adding exported predicted labels to a json file to cover cases where data is gathered from scratch, so predictions aren't lost?\n",
    "- Implement checks if all required data exists and for calculateable data, calculate it if missing\n",
    "- Implement predetermined breaking points (Sollbruchstellen) if required data is missing or the script can't continue for some reason\n",
    "- Issues can be deleted... should there be a check if any issues got deleted, so they get deleted from the data json file as well? and how to handle edits?\n",
    "    - when fetching issues one could add a dict with all issue numbers, which has the key set to True if the issue already exists (opposed to merely continuing with the next one) or after I got fetched, and all issues, whichs key is still False afterwards got deleted... we could add a field that saves this info and also an option in a config to delete all of these issues\n",
    "- Should new issues be fetched every time? (new issues meaning issues with a higher number than the highest one in current data set)\n",
    "- What if all issues are unlabeled? Then fine-tuning should be skipped... make sure it works like that!\n",
    "- Issue data can now either be a list or dictionary... come up with a way to work with that or enforce choosing either!\n",
    "- Implement that connection to github repo isn't required if issues_data (json file containing it) is given\n",
    "- GitHub:\n",
    "    - README\n",
    "    - Requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports required for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyGithub\n",
    "from github import Github, RateLimitExceededException\n",
    "from github.Repository import Repository\n",
    "# conda install python-dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import logging.config\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pandas\n",
    "import pandas as pd\n",
    "# conda install numpy\n",
    "import numpy as np\n",
    "# conda install toml\n",
    "import toml\n",
    "# conda install PyYAML\n",
    "import yaml\n",
    "# conda install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports required for dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports required for Issue Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# pip install transformers[torch]\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# from sklearn.preprocessing import LabelEncoder # TODO: implement LabelEncoder instead of using pandas.Series.cat.codes since that is less error prone - TODO: verify that the encodings stay the same\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from functools import partial\n",
    "import unicodedata as ud\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic setup things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL = \"full\"\n",
    "# PARTIAL = \"partial\"\n",
    "# ISSUES = \"issues\"\n",
    "# PRS = \"pull_requests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function_requirements = {\n",
    "#     \"first_prediction\": {\n",
    "#         ISSUES: {\"number\": FULL, \"title\": FULL, \"user\": FULL, \"description\": PARTIAL, \"labels\": PARTIAL},\n",
    "#         PRS: {}\n",
    "#     },\n",
    "#     \"prediction_without_duplicates\": {\n",
    "#         ISSUES: {\"number\": FULL, \"title\": FULL, \"user\": FULL, \"description\": PARTIAL, \"labels\": PARTIAL, \"predicted_label\": PARTIAL},\n",
    "#         PRS: {}\n",
    "#     },\n",
    "#     \"function_3\": {\"data_key_1\": \"full\", \"data_key_4\": \"partial\"},\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_availability_dicts = {\n",
    "#     ISSUES: {\n",
    "#         \"fully_available\": data_type_1_fully_available,\n",
    "#         \"partially_available\": data_type_1_partially_available,\n",
    "#     },\n",
    "#     PRS: {\n",
    "#         \"fully_available\": data_type_2_fully_available,\n",
    "#         \"partially_available\": data_type_2_partially_available,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_function_executability(function_requirements, data_availability_dicts):\n",
    "#     executable_functions = {}\n",
    "\n",
    "#     for func, data_requirements in function_requirements.items():\n",
    "#         is_executable = True  # Assume executable unless a requirement fails\n",
    "\n",
    "#         for data_type, requirements in data_requirements.items():\n",
    "#             fully_available = data_availability_dicts[data_type][\"fully_available\"]\n",
    "#             partially_available = data_availability_dicts[data_type][\"partially_available\"]\n",
    "\n",
    "#             for key, required_availability in requirements.items():\n",
    "#                 if required_availability == \"full\":\n",
    "#                     if not fully_available.get(key, False):\n",
    "#                         is_executable = False\n",
    "#                         break\n",
    "#                 elif required_availability == \"partial\":\n",
    "#                     if not (fully_available.get(key, False) or partially_available.get(key, False)):\n",
    "#                         is_executable = False\n",
    "#                         break\n",
    "\n",
    "#             if not is_executable:\n",
    "#                 break  # Stop checking other data types if one fails\n",
    "\n",
    "#         executable_functions[func] = \"executable\" if is_executable else \"not_executable\"\n",
    "\n",
    "#     return executable_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = check_function_executability(function_requirements, data_availability_dicts)\n",
    "\n",
    "# for func, status in result.items():\n",
    "#     logger.info(f\"{func}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGING_LEVEL = logging.INFO\n",
    "\n",
    "# The following two values can only be changed here as otherwise the cofigs couldn\"t be loaded\n",
    "CONFIG_FOLDER = \"config\"\n",
    "CONFIG_FILE_NAME = \"config.toml\"\n",
    "\n",
    "BUG = \"bug\"\n",
    "ENHANCEMENT = \"enhancement\"\n",
    "SUPPORT = \"support\"\n",
    "\n",
    "PREDICTION_BUG_LABEL = BUG\n",
    "PREDICTION_ENHANCEMENT_LABEL = ENHANCEMENT\n",
    "PREDICTION_SUPPORT_LABEL = SUPPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "logging.basicConfig(level=LOGGING_LEVEL)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "logging.info(f\"CUDA Available: {cuda_available}\")\n",
    "# Get the name of the GPU\n",
    "if cuda_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    logging.debug(f\"GPU: {gpu_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOKS_FOLDER = \"notebooks\"\n",
    "NOTEBOOKS_FOLDER = os.path.basename(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOKS_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_last_folder(path):\n",
    "    # Remove the last folder from the path\n",
    "    return os.path.dirname(path)\n",
    "\n",
    "# # Since this notebook is in the notebooks folder, we need to remove the notebooks folder from the path to get the project folder\n",
    "# PROJECT_FOLDER = \"\"\n",
    "# cwd = os.getcwd()\n",
    "# if (NOTEBOOKS_FOLDER in cwd): # TODO: only look at last folder instead of whole path? (as theoretically the name of the notebook folder could be part of the prior path)\n",
    "#     PROJECT_FOLDER = remove_last_folder(os.getcwd())\n",
    "# else:\n",
    "#     PROJECT_FOLDER = cwd\n",
    "\n",
    "PROJECT_FOLDER = remove_last_folder(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GITHUB_PAT from .env file\n",
    "GITHUB_PAT = os.getenv(\"GITHUB_PAT\", \"\")\n",
    "\n",
    "# GITHUB_TOKEN = rf\"{remove_last_folder(PROJECT_FOLDER)}\\private\\pat.txt\" # TODO: remove and make sure that better solution does work\n",
    "\n",
    "# with open(GITHUB_TOKEN) as file:\n",
    "#     GITHUB_PAT = file.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic config (config.toml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_repository\n",
    "USER_NAME = \"vaadin\" \n",
    "REPO_NAME = \"flow\"\n",
    "FETCH_NEW_ISSUES = True\n",
    "\n",
    "# logging\n",
    "LOGGER_NAME = \"StandardLogger\"\n",
    "LOG_FILE_NAME = \"app.log\"\n",
    "\n",
    "# folder_structure\n",
    "DATA_FOLDER = \"data\"\n",
    "MODEL_FOLDER = \"model\"\n",
    "LOGS_FOLDER = \"logs\"\n",
    "RESULTS_FOLDER = \"results\"\n",
    "\n",
    "# config_files\n",
    "LOGGING_CONFIG_FILE_NAME = \"logging_config.yaml\"\n",
    "REPO_DATA_CONFIG_FILE_NAME = \"repo_data_config.toml\"\n",
    "LABEL_CONFIG_FILE_NAME = \"label_config.toml\"\n",
    "ML_CONFIG_FILE_NAME = \"ml_config.toml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default values in case keys are missing\n",
    "DEFAULT_CONFIG = {\n",
    "    \"selected_repository\": {\n",
    "        \"user_name\": \"vaadin\",\n",
    "        \"repo_name\": \"flow\",\n",
    "        \"fetch_new_issues\": True\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"logger_name\": \"StandardLogger\",\n",
    "        \"log_file_name\": \"app.log\"\n",
    "    },\n",
    "    \"folder_structure\": {\n",
    "        \"data_folder\": \"data\",\n",
    "        \"logs_folder\": \"logs\",\n",
    "        \"model_folder\": \"models\",\n",
    "        \"notebooks_folder\": \"notebooks\",\n",
    "        \"results_folder\": \"results\"\n",
    "    },\n",
    "    \"config_files\": {\n",
    "        \"logging_config_file_name\": \"logging_config.yaml\",\n",
    "        \"repo_data_config_file_name\": \"repo_data.toml\",\n",
    "        \"label_config_file_name\": \"label_config.toml\",\n",
    "        \"ml_config_file_name\": \"ml_config.toml\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_main_config(config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, CONFIG_FILE_NAME)):\n",
    "    global USER_NAME, REPO_NAME, FETCH_NEW_ISSUES\n",
    "    global LOGGER_NAME, LOG_FILE_NAME\n",
    "    global DATA_FOLDER, LOGS_FOLDER, MODEL_FOLDER, NOTEBOOKS_FOLDER, RESULTS_FOLDER\n",
    "    global LOGGING_CONFIG_FILE_NAME, REPO_DATA_CONFIG_FILE_NAME, LABEL_CONFIG_FILE_NAME, ML_CONFIG_FILE_NAME\n",
    "\n",
    "    if not os.path.exists(config_file):\n",
    "        logging.warning(f\"Config file {config_file} not found. Using default values.\")\n",
    "        config_data = DEFAULT_CONFIG\n",
    "    else:\n",
    "        with open(config_file, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "            config_data = toml.load(file)\n",
    "\n",
    "    # Handle missing sections with default values\n",
    "    config_data = {**DEFAULT_CONFIG, **config_data}  # Merge with defaults\n",
    "\n",
    "    selected_repo = config_data.get(\"selected_repository\", {})\n",
    "    USER_NAME = selected_repo.get(\"user_name\", DEFAULT_CONFIG[\"selected_repository\"][\"user_name\"])\n",
    "    REPO_NAME = selected_repo.get(\"repo_name\", DEFAULT_CONFIG[\"selected_repository\"][\"repo_name\"])\n",
    "    FETCH_NEW_ISSUES = selected_repo.get(\"fetch_new_issues\", DEFAULT_CONFIG[\"selected_repository\"][\"fetch_new_issues\"])\n",
    "\n",
    "    # Logging section\n",
    "    logging_config = config_data.get(\"logging\", {})\n",
    "    LOGGER_NAME = logging_config.get(\"logger_name\", DEFAULT_CONFIG[\"logging\"][\"logger_name\"])\n",
    "    LOG_FILE_NAME = logging_config.get(\"log_file_name\", DEFAULT_CONFIG[\"logging\"][\"log_file_name\"])\n",
    "\n",
    "    # Folder structure\n",
    "    folder_structure = config_data.get(\"folder_structure\", {})\n",
    "    DATA_FOLDER = folder_structure.get(\"data_folder\", DEFAULT_CONFIG[\"folder_structure\"][\"data_folder\"])\n",
    "    LOGS_FOLDER = folder_structure.get(\"logs_folder\", DEFAULT_CONFIG[\"folder_structure\"][\"logs_folder\"])\n",
    "    MODEL_FOLDER = folder_structure.get(\"model_folder\", DEFAULT_CONFIG[\"folder_structure\"][\"model_folder\"])\n",
    "    NOTEBOOKS_FOLDER = folder_structure.get(\"notebooks_folder\", DEFAULT_CONFIG[\"folder_structure\"][\"notebooks_folder\"])\n",
    "    RESULTS_FOLDER = folder_structure.get(\"results_folder\", DEFAULT_CONFIG[\"folder_structure\"][\"results_folder\"])\n",
    "\n",
    "    # Config files\n",
    "    config_files = config_data.get(\"config_files\", {})\n",
    "    LOGGING_CONFIG_FILE_NAME =  config_files.get(\"logging_config_file_name\", DEFAULT_CONFIG[\"config_files\"][\"logging_config_file_name\"])\n",
    "    REPO_DATA_CONFIG_FILE_NAME = config_files.get(\"repo_data_config_file_name\", DEFAULT_CONFIG[\"config_files\"][\"repo_data_config_file_name\"])\n",
    "    LABEL_CONFIG_FILE_NAME = config_files.get(\"label_config_file_name\", DEFAULT_CONFIG[\"config_files\"][\"label_config_file_name\"])\n",
    "    ML_CONFIG_FILE_NAME = config_files.get(\"ml_config_file_name\", DEFAULT_CONFIG[\"config_files\"][\"ml_config_file_name\"])\n",
    "\n",
    "    logging.info(\"Configuration successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_main_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "os.makedirs(LOGS_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODEL_FOLDER, exist_ok=True)\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "with open(LOG_FILE_NAME, 'a', encoding=\"utf-8-sig\"):\n",
    "    os.utime(LOG_FILE_NAME, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML parameters (Do not change as this can negatively impact or break the model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TITLE_LENGTH = 30\n",
    "MAX_BODY_LENGTH = 170\n",
    "PUNCTUATIONS = r'!\"$%&\\()*,/:;<=>[\\\\]^`{|}~+#@-`'\n",
    "ISSUE_REGEX = r\"#[0-9]+\"\n",
    "FUNCTION_REGEX = r\"[a-zA-Z][a-zA-Z0-9_.]*\\([a-zA-Z0-9_, ]*\\)\"\n",
    "ASCII_REGEX = r\"[^\\x00-\\x7f]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FINETUNED_MODEL = False\n",
    "PRETRAINED_MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "FINETUNED_MODEL_FOLDER = \"roberta\"\n",
    "FINETUNED_MODEL_FILE_NAME = \"\"\n",
    "\n",
    "EVALUATION = True\n",
    "TEST = True\n",
    "\n",
    "VALIDATION_SPLIT_PERCENTAGE = 0.2\n",
    "RANDOM_STATE = None\n",
    "\n",
    "NUMBER = \"number\"\n",
    "\n",
    "LABEL = \"labels\"\n",
    "TIME = \"created_at\"\n",
    "REPO = \"repository_url\"\n",
    "TITLE = \"title\"\n",
    "BODY = \"description\"\n",
    "AUTHOR = \"author_association\"\n",
    "URL = \"url\"\n",
    "PREDICTED_LABEL = \"predicted_label\"\n",
    "\n",
    "LABEL_COL = \"labels\"\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "EXPORT_ML_DATA_CSVS = False\n",
    "EXPORT_FINETUNED_MODEL = False # TODO: Implement!!!\n",
    "EXPORT_PREDICTIONS = True\n",
    "UPDATE_ISSUE_DATA = True\n",
    "UPDATE_GITHUB_LABELS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default ML configuration\n",
    "DEFAULT_ML_CONFIG = {\n",
    "    \"model\": {\n",
    "        \"use_finetuned_model\": False,\n",
    "        \"pretrained_model_file_name\": \"pytorch_model.bin\",\n",
    "        \"finetuned_model_folder\": \"\",\n",
    "        \"finetuned_model_file_name\": \"\"\n",
    "    },\n",
    "    \"mode\": {\n",
    "        \"evaluation\": True,\n",
    "        \"test\": True,\n",
    "        \"validation_split_percentage\": 0.2,\n",
    "        \"random_state\": None\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"keys\": {\n",
    "            \"number\": \"number\",\n",
    "            \"label\": \"labels\",\n",
    "            \"time\": \"created_at\",\n",
    "            \"repo\": \"repository_url\",\n",
    "            \"title\": \"title\",\n",
    "            \"body\": \"description\",\n",
    "            \"author\": \"author_association\",\n",
    "            \"url\": \"url\",\n",
    "            \"predicted_label\": \"predicted_label\",\n",
    "            \"label_col\": \"labels\",\n",
    "            \"text_col\": \"text\"\n",
    "        },\n",
    "        # \"preprocessing\": {\n",
    "        #     \"max_title_length\": 30,\n",
    "        #     \"max_body_length\": 170,\n",
    "        #     \"punctuations\": '!\"$%&\\\\()*,/:;<=>[\\\\]^`{|}~+#@-`',\n",
    "        #     \"issue_regex\": r\"#[0-9]+\",\n",
    "        #     \"function_regex\": r\"[a-zA-Z][a-zA-Z0-9_.]*\\([a-zA-Z0-9_, ]*\\)\",\n",
    "        #     \"ascii_regex\": r\"[^\\x00-\\x7f]\"\n",
    "        # },\n",
    "        \"export\": {\n",
    "            \"export_ml_data_csvs\": True,\n",
    "            \"export_finetuned_model\": False,\n",
    "            \"export_predictions\": True,\n",
    "            \"update_issue_data\": False,\n",
    "            \"update_github_labels\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Global variables (initialized with default values)\n",
    "USE_FINETUNED_MODEL = DEFAULT_ML_CONFIG[\"model\"][\"use_finetuned_model\"]\n",
    "PRETRAINED_MODEL_FILE_NAME = DEFAULT_ML_CONFIG[\"model\"][\"pretrained_model_file_name\"]\n",
    "FINETUNED_MODEL_FOLDER = DEFAULT_ML_CONFIG[\"model\"][\"finetuned_model_folder\"]\n",
    "FINETUNED_MODEL_FILE_NAME = DEFAULT_ML_CONFIG[\"model\"][\"finetuned_model_file_name\"]\n",
    "\n",
    "EVALUATION = DEFAULT_ML_CONFIG[\"mode\"][\"evaluation\"]\n",
    "TEST = DEFAULT_ML_CONFIG[\"mode\"][\"test\"]\n",
    "VALIDATION_SPLIT_PERCENTAGE = DEFAULT_ML_CONFIG[\"mode\"][\"validation_split_percentage\"]\n",
    "RANDOM_STATE = DEFAULT_ML_CONFIG[\"mode\"][\"random_state\"]\n",
    "\n",
    "NUMBER = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"number\"]\n",
    "LABEL = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"label\"]\n",
    "TIME = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"time\"]\n",
    "REPO = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"repo\"]\n",
    "TITLE = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"title\"]\n",
    "BODY = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"body\"]\n",
    "AUTHOR = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"author\"]\n",
    "URL = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"url\"]\n",
    "PREDICTED_LABEL = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"predicted_label\"]\n",
    "\n",
    "LABEL_COL = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"label_col\"]\n",
    "TEXT_COL = DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"text_col\"]\n",
    "\n",
    "# MAX_TITLE_LENGTH = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"max_title_length\"]\n",
    "# MAX_BODY_LENGTH = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"max_body_length\"]\n",
    "# PUNCTUATIONS = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"punctuations\"]\n",
    "# ISSUE_REGEX = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"issue_regex\"]\n",
    "# FUNCTION_REGEX = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"function_regex\"]\n",
    "# ASCII_REGEX = DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"ascii_regex\"]\n",
    "\n",
    "EXPORT_ML_DATA_CSVS = DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_ml_data_csvs\"]\n",
    "EXPORT_FINETUNED_MODEL = DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_finetuned_model\"]\n",
    "EXPORT_PREIDCTIONS = DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_predictions\"]\n",
    "UPDATE_ISSUE_DATA = DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"update_issue_data\"]\n",
    "UPDATE_GITHUB_LABELS = DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"update_github_labels\"]\n",
    "\n",
    "def load_ml_config(config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, ML_CONFIG_FILE_NAME)):\n",
    "    global USE_FINETUNED_MODEL, PRETRAINED_MODEL_FILE_NAME, FINETUNED_MODEL_FOLDER, FINETUNED_MODEL_FILE_NAME\n",
    "    global EVALUATION, TEST, VALIDATION_SPLIT_PERCENTAGE, RANDOM_STATE\n",
    "    global NUMBER, LABEL, TIME, REPO, TITLE, BODY, AUTHOR, URL, PREDICTED_LABEL, LABEL_COL, TEXT_COL\n",
    "    # global MAX_TITLE_LENGTH, MAX_BODY_LENGTH, PUNCTUATIONS, ISSUE_REGEX, FUNCTION_REGEX, ASCII_REGEX\n",
    "    global EXPORT_ML_DATA_CSVS, EXPORT_FINETUNED_MODEL, EXPORT_PREDICTIONS, UPDATE_ISSUE_DATA, UPDATE_GITHUB_LABELS\n",
    "\n",
    "    if not os.path.exists(config_file):\n",
    "        logging.warning(f\"Config file {config_file} not found. Using default values.\")\n",
    "        config_data = DEFAULT_ML_CONFIG\n",
    "    else:\n",
    "        with open(config_file, 'r', encoding=\"utf-8-sig\") as file:\n",
    "            config_data = toml.load(file)\n",
    "        # Merge with default configuration\n",
    "        config_data = {**DEFAULT_ML_CONFIG, **config_data}\n",
    "\n",
    "    # Model section\n",
    "    USE_FINETUNED_MODEL = config_data[\"model\"].get(\"use_finetuned_model\", DEFAULT_ML_CONFIG[\"model\"][\"use_finetuned_model\"])\n",
    "    PRETRAINED_MODEL_FILE_NAME = config_data[\"model\"].get(\"pretrained_model_file_name\", DEFAULT_ML_CONFIG[\"model\"][\"pretrained_model_file_name\"])\n",
    "    FINETUNED_MODEL_FOLDER = config_data[\"model\"].get(\"finetuned_model_folder\", DEFAULT_ML_CONFIG[\"model\"][\"finetuned_model_folder\"])\n",
    "    FINETUNED_MODEL_FILE_NAME = config_data[\"model\"].get(\"finetuned_model_file_name\", DEFAULT_ML_CONFIG[\"model\"][\"finetuned_model_file_name\"])\n",
    "\n",
    "    # Mode section\n",
    "    EVALUATION = config_data[\"mode\"].get(\"evaluation\", DEFAULT_ML_CONFIG[\"mode\"][\"evaluation\"])\n",
    "    TEST = config_data[\"mode\"].get(\"test\", DEFAULT_ML_CONFIG[\"mode\"][\"test\"])\n",
    "    VALIDATION_SPLIT_PERCENTAGE = config_data[\"mode\"].get(\"validation_split_percentage\", DEFAULT_ML_CONFIG[\"mode\"][\"validation_split_percentage\"])\n",
    "    RANDOM_STATE = config_data[\"mode\"].get(\"random_state\", DEFAULT_ML_CONFIG[\"mode\"][\"random_state\"])\n",
    "\n",
    "    # Data section\n",
    "    data_config = config_data.get(\"data\", {})\n",
    "    data_config = {**DEFAULT_ML_CONFIG[\"data\"], **data_config}\n",
    "\n",
    "    # Data keys\n",
    "    data_keys = data_config.get(\"keys\", {})\n",
    "    NUMBER = data_keys.get(\"number\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"number\"])\n",
    "    LABEL = data_keys.get(\"label\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"label\"])\n",
    "    TIME = data_keys.get(\"time\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"time\"])\n",
    "    REPO = data_keys.get(\"repo\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"repo\"])\n",
    "    TITLE = data_keys.get(\"title\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"title\"])\n",
    "    BODY = data_keys.get(\"body\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"body\"])\n",
    "    AUTHOR = data_keys.get(\"author\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"author\"])\n",
    "    URL = data_keys.get(\"url\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"url\"])\n",
    "    PREDICTED_LABEL = data_keys.get(\"predicted_label\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"predicted_label\"])\n",
    "    LABEL_COL = data_keys.get(\"label_col\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"label_col\"])\n",
    "    TEXT_COL = data_keys.get(\"text_col\", DEFAULT_ML_CONFIG[\"data\"][\"keys\"][\"text_col\"])\n",
    "    \n",
    "\n",
    "    # # Data preprocessing\n",
    "    # data_preprocessing = data_config.get(\"preprocessing\", {})\n",
    "    # MAX_TITLE_LENGTH = data_preprocessing.get(\"max_title_length\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"max_title_length\"])\n",
    "    # MAX_BODY_LENGTH = data_preprocessing.get(\"max_body_length\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"max_body_length\"])\n",
    "    # PUNCTUATIONS = data_preprocessing.get(\"punctuations\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"punctuations\"])\n",
    "    # ISSUE_REGEX = data_preprocessing.get(\"issue_regex\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"issue_regex\"])\n",
    "    # FUNCTION_REGEX = data_preprocessing.get(\"function_regex\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"function_regex\"])\n",
    "    # ASCII_REGEX = data_preprocessing.get(\"ascii_regex\", DEFAULT_ML_CONFIG[\"data\"][\"preprocessing\"][\"ascii_regex\"])\n",
    "\n",
    "    # Data export\n",
    "    data_export = data_config.get(\"export\", {})\n",
    "    EXPORT_ML_DATA_CSVS = data_export.get(\"export_ml_data_csvs\", DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_ml_data_csvs\"])\n",
    "    EXPORT_FINETUNED_MODEL = data_export.get(\"export_finetuned_model\", DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_finetuned_model\"])\n",
    "    EXPORT_PREDICTIONS = data_export.get(\"export_predictions\", DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"export_predictions\"])\n",
    "    UPDATE_ISSUE_DATA = data_export.get(\"update_issue_data\", DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"update_issue_data\"])\n",
    "    UPDATE_GITHUB_LABELS = data_export.get(\"update_github_labels\", DEFAULT_ML_CONFIG[\"data\"][\"export\"][\"update_github_labels\"])\n",
    "\n",
    "    logging.info(\"ML configuration successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ml_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NECESSARY_COLUMNS = [LABEL, TIME, AUTHOR, REPO, TITLE, BODY, URL, PREDICTED_LABEL]\n",
    "USEFUL_COLUMNS = [NUMBER] + NECESSARY_COLUMNS\n",
    "\n",
    "ISSUE_REGEX = re.compile(ISSUE_REGEX)\n",
    "FUNCTION_REGEX = re.compile(FUNCTION_REGEX)\n",
    "ASCII_REGEX = re.compile(ASCII_REGEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(\n",
    "        default_path=os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LOGGING_CONFIG_FILE_NAME),\n",
    "        default_level=LOGGING_LEVEL,\n",
    "        env_key=\"LOG_CFG\"):\n",
    "    \"\"\"\n",
    "    Setup logging configuration.\n",
    "    \"\"\"\n",
    "    config_path = default_path\n",
    "    new_config_path = os.getenv(env_key, None)\n",
    "    if new_config_path:\n",
    "        config_path = new_config_path\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "            config = yaml.safe_load(file.read())\n",
    "        logging.config.dictConfig(config)\n",
    "    else:\n",
    "        logging.basicConfig(level=default_level)\n",
    "        logging.warning(f\"Logging configuration file not found at {config_path}, using basic configuration.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(LOGGER_NAME)\n",
    "    logger.debug(\"This is a test debug message\")\n",
    "    logger.info(\"This is a test info message\")\n",
    "    logger.warning(\"This is a test warning message\")\n",
    "    logger.error(\"This is a test error message\")\n",
    "    logger.critical(\"This is a test critical message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to load other configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary checks for Config file\n",
    "if both github data json file and label lists are complete there is no need for GITHUB_PAT as a github connection isn\"t needed (there should be a bool so one can update those 2 though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RepoDataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepoDataHandler:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_name: str = \"\",\n",
    "        repository_name: str = \"\",\n",
    "        token: str = GITHUB_PAT,\n",
    "        github_api: Github = None,\n",
    "        repo: Repository = None,\n",
    "        repo_data_file_path: str = \"\",\n",
    "        repo_data_file_name: str = \"\",\n",
    "        # issues_data: dict = {}, # dict version\n",
    "        issues_data: list = [], # list version\n",
    "        issues_last_fetch: datetime = None,\n",
    "        # pull_requests_data: dict = {}, # dict version\n",
    "        pull_requests_data: list = [], # list version\n",
    "        issue_pr_map: dict = {},\n",
    "        issues_updated_since_last_pr_map: bool = False,\n",
    "        prs_updated_since_last_pr_map: bool = False,\n",
    "        bug_labels: list = [],\n",
    "        enhancement_labels: list = [],\n",
    "        support_labels: list = [],\n",
    "        prediction_bug_label: str = PREDICTION_BUG_LABEL,\n",
    "        prediction_enhancement_label: str = PREDICTION_ENHANCEMENT_LABEL,\n",
    "        prediction_support_label: str = PREDICTION_SUPPORT_LABEL,\n",
    "        predictions_df: pd.DataFrame = None\n",
    "    ):\n",
    "        self.user_name: str = user_name\n",
    "        self.repository_name: str = repository_name\n",
    "        self.token: str = token\n",
    "        self.github_api: Github = github_api\n",
    "        self.repo: Repository = repo\n",
    "        # Initialize the repo data file path and name\n",
    "        self.repo_data_file_path: str = \"\"\n",
    "        self.repo_data_file_name: str = \"\"\n",
    "        if(repo_data_file_path): self.repo_data_file_path: str = repo_data_file_path\n",
    "        else:\n",
    "            if(repo_data_file_name):\n",
    "                file_path = os.path.join(PROJECT_FOLDER, DATA_FOLDER, repo_data_file_name)\n",
    "                if(os.path.exists(file_path)): self.repo_data_file_path: str = file_path\n",
    "            else:\n",
    "                self.repo_data_file_path: str = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}_repo_data.json\") # TODO: come up with better handling of this\n",
    "        if(repo_data_file_name): self.repo_data_file_name = repo_data_file_name\n",
    "        else:\n",
    "            if(self.repo_data_file_path):\n",
    "                if(os.path.exists(self.repo_data_file_path)):\n",
    "                    self.repo_data_file_name = os.path.basename(self.repo_data_file_path)\n",
    "            else:\n",
    "                self.repo_data_file_name = f\"{self.user_name}_{self.repository_name}_repo_data.json\"\n",
    "        \n",
    "        self.issues_data: dict = issues_data\n",
    "        # self.issues_last_fetch: datetime = issues_last_fetch # TODO: implement or remove\n",
    "        self.pull_requests_data: dict = pull_requests_data\n",
    "        self.issue_pr_map: dict = issue_pr_map\n",
    "        # self.issues_updated_since_last_pr_map: bool = issues_updated_since_last_pr_map # TODO: implement or remove - also: Instead check length before and after fetching?\n",
    "        # self.prs_updated_since_last_pr_map: bool = prs_updated_since_last_pr_map # TODO: implement or remove - also: Instead check length before and after fetching?\n",
    "        self.bug_labels: list = bug_labels\n",
    "        self.enhancement_labels: list = enhancement_labels\n",
    "        self.support_labels: list = support_labels\n",
    "        self.prediction_bug_label = prediction_bug_label\n",
    "        self.prediction_enhancement_label = prediction_enhancement_label\n",
    "        self.prediction_support_label = prediction_support_label\n",
    "        self.results_df = None\n",
    "\n",
    "        if not self.repo: self.setup_github_api()\n",
    "\n",
    "        # In case a connection to the repository is provided, but not the user_name and repository_name\n",
    "        if (not (self.user_name and self.repository_name) and self.repo):\n",
    "            self.user_name = self.repo.owner.login\n",
    "            self.repository_name = self.repo.name\n",
    "\n",
    "        self.fully_available_issue_data = {\n",
    "            \"number\": False,\n",
    "            \"title\": False,\n",
    "            \"state\": False,\n",
    "            \"created_at\": False,\n",
    "            \"closed_at\": False,\n",
    "            \"user\": False,\n",
    "            \"comments\": False,\n",
    "            \"description\": False,\n",
    "            \"description_length\": False,\n",
    "            \"labels\": False,\n",
    "            \"predicted_label\": False,\n",
    "            \"author_association\": False\n",
    "        }\n",
    "\n",
    "        self.fully_available_pr_data = {\n",
    "            \"number\": False,\n",
    "            \"title\": False,\n",
    "            \"state\": False,\n",
    "            \"created_at\": False,\n",
    "            \"merged_at\": False,\n",
    "            \"closed_at\": False,\n",
    "            \"user\": False,\n",
    "            \"comments\": False,\n",
    "            \"review_comments\": False,\n",
    "            \"description\": False,\n",
    "            \"description_length\": False,\n",
    "            \"additions\": False,\n",
    "            \"deletions\": False,\n",
    "            \"changed_files\": False,\n",
    "            \"files\": False,\n",
    "            \"commits\": False,\n",
    "            \"labels\": False,\n",
    "            \"mergeable_state\": False,\n",
    "            \"files_changed\": {\n",
    "                \"filename\": False,\n",
    "                \"additions\": False,\n",
    "                \"deletions\": False\n",
    "            },\n",
    "            \"commit_data\": {\n",
    "                \"sha\": False,\n",
    "                \"author\": False,\n",
    "                \"date\": False,\n",
    "                \"message\": False\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.partially_available_issue_data = self.fully_available_issue_data.copy()\n",
    "\n",
    "        self.partially_available_pr_data = self.fully_available_pr_data.copy()\n",
    "\n",
    "    # def setup_github_api(self, token: str = None): # TODO: doesn\"t work rn due to self.token (if changed back please remember to rmeove \"self.\" before token)\n",
    "    def setup_github_api(self):\n",
    "        if self.repo:\n",
    "            logger.info(f\"Connected to {self.repo.full_name}\")\n",
    "            return\n",
    "        if(self.token): self.github_api = Github(self.token)\n",
    "        if(not self.github_api): raise Exception(\"No token for or connection to the Github API provided\")\n",
    "        try:\n",
    "            self.repo = self.github_api.get_repo(f\"{self.user_name}/{self.repository_name}\")\n",
    "            logger.info(f\"Connected to {self.repo.full_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error connecting to the repository {self.user_name}/{self.repository_name}: {e}\")\n",
    "\n",
    "    def load_repo_data_config(self, config_file: str = \"\"):\n",
    "\n",
    "        # if self.issues_data:\n",
    "        #     logger.info(\"Loading repo data config not necessary as issue data already given.\")\n",
    "        #     return\n",
    "        \n",
    "        if config_file: logger.debug(f\"Trying to load repo data file configuration from {config_file}\")\n",
    "        \n",
    "        else:\n",
    "            config_file = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, REPO_DATA_CONFIG_FILE_NAME)\n",
    "            logger.debug(f'No config file given. Trying to load config from \"{os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, REPO_DATA_CONFIG_FILE_NAME)}\"')\n",
    "\n",
    "        repo_key = f\"{self.user_name}/{self.repository_name}\"\n",
    "        \n",
    "        if not os.path.exists(config_file):\n",
    "            logger.warning(f'No repo data file path provided. Assuming default \"{os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{repo_key}_repo_data.json\")}\"!')\n",
    "            config_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{repo_key}_repo_data.json\")\n",
    "            if not os.path.exists(config_file):\n",
    "                repo_data_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{repo_key}_repo_data.json\")\n",
    "                logger.warning(f\"{config_file} doesn't exist either. Trying to load data directly from default data file location \\\"{repo_data_file}\\\".\")\n",
    "                if not os.path.exists(repo_data_file):\n",
    "                    logger.warning(f\"Repo data file doesn't exist either. Fetching issues...\")\n",
    "                    self.fetch_issues()\n",
    "\n",
    "        else:\n",
    "            with open(config_file, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "                config_data = toml.load(file)\n",
    "    \n",
    "            # Find the configuration for the current user_name/repo_name\n",
    "            if repo_key not in config_data:\n",
    "                repo_data_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{repo_key}_repo_data.json\")\n",
    "                logger.warning(f'No configuration found for {repo_key} in the config file. Assuming default data file location \"{repo_data_file}\".')\n",
    "                return\n",
    "                \n",
    "            repo_data_file = config_data[repo_key][\"repo_data_file\"]\n",
    "            logger.debug(f\"Loaded repo data file configuration for {repo_key}: {repo_data_file}\")\n",
    "    \n",
    "            if(repo_data_file == os.path.basename(repo_data_file)): # If it is just a file name, not full path\n",
    "                self.repo_data_file_name = repo_data_file\n",
    "                self.repo_data_file_path = os.path.join(PROJECT_FOLDER, DATA_FOLDER, self.repo_data_file_name)\n",
    "            else:\n",
    "                self.repo_data_file_name = os.path.basename(repo_data_file)\n",
    "                self.repo_data_file_path = repo_data_file\n",
    "\n",
    "    def load_data(self, repo_data_json_file: str = \"\"):\n",
    "        logger.debug(f\"self.repo_data_file_name: {self.repo_data_file_name}\")\n",
    "        if(not repo_data_json_file):\n",
    "            logger.debug(\"No file given, loading file from default file path\")\n",
    "            if(self.repo_data_file_path):\n",
    "                logger.debug(f\"Loading data from self.repo_data_file_path: {self.repo_data_file_path}\")\n",
    "                repo_data_json_file = self.repo_data_file_path\n",
    "            elif(self.repo_data_file_name):\n",
    "                logger.debug(f\"Loading data from self.repo_data_file_name: {self.repo_data_file_name}\")\n",
    "                repo_data_json_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, self.repo_data_file_name)\n",
    "            else:\n",
    "                logger.info(\"No info on data file given. Can't load any data.\")\n",
    "                return\n",
    "            \n",
    "\n",
    "        logger.debug(f\"Repo data json file: {repo_data_json_file}\")\n",
    "\n",
    "        try:\n",
    "            with open(repo_data_json_file, encoding=\"utf-8-sig\") as input_file:\n",
    "                data = json.load(input_file)\n",
    "                self.issues_data = data[\"issues\"]\n",
    "                self.pull_requests_data = data[\"pull_requests\"]\n",
    "                self.issue_pr_map = data[\"issue_pr_map\"]\n",
    "        except FileNotFoundError:\n",
    "            logger.info(\"No existing data file found.\")\n",
    "\n",
    "    def _load_label_config(self, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        if not os.path.exists(config_file):\n",
    "            raise FileNotFoundError(f\"Config file {config_file} not found.\")\n",
    "\n",
    "        with open(config_file, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "            config_data = toml.load(file)\n",
    "\n",
    "        # Find the configuration for the current user/repo\n",
    "        repo_key = f\"{self.user_name}/{self.repository_name}\"\n",
    "        if repo_key not in config_data:\n",
    "            raise ValueError(f\"No configuration found for {repo_key} in the config file.\")\n",
    "\n",
    "        return config_data\n",
    "        \n",
    "    def load_label_config(self, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        config_data = self._load_label_config(config_file=config_file)\n",
    "\n",
    "        repo_key = f\"{self.user_name}/{self.repository_name}\"\n",
    "        labels = config_data[repo_key]\n",
    "        logger.debug(f\"Loaded label configuration for {repo_key}: {labels}\")\n",
    "        self.bug_labels = labels.get(\"bug_labels\", [])\n",
    "        self.enhancement_labels = labels.get(\"enhancement_labels\", [])\n",
    "        self.support_labels = labels.get(\"support_labels\", [])\n",
    "        self.prediction_bug_label = labels.get(\"prediction_bug_label\", PREDICTION_BUG_LABEL)\n",
    "        self.prediction_enhancement_label = labels.get(\"prediction_enhancement_label\", PREDICTION_ENHANCEMENT_LABEL)\n",
    "        self.prediction_support_label = labels.get(\"prediction_support_label\", PREDICTION_SUPPORT_LABEL)\n",
    "\n",
    "    def load_label_lists(self, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        config_data = self._load_label_config()\n",
    "\n",
    "        repo_key = f\"{self.user_name}/{self.repository_name}\"\n",
    "        labels = config_data[repo_key]\n",
    "        logger.debug(f\"Loaded label configuration for {repo_key}: {labels}\")\n",
    "        self.bug_labels = labels.get(\"bug_labels\", [])\n",
    "        self.enhancement_labels = labels.get(\"enhancement_labels\", [])\n",
    "        self.support_labels = labels.get(\"support_labels\", [])\n",
    "        \n",
    "    def load_prediction_labels(self, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        config_data = self._load_label_config()\n",
    "\n",
    "        repo_key = f\"{self.user_name}/{self.repository_name}\"\n",
    "        labels = config_data[repo_key]\n",
    "        logger.debug(f\"Loaded label configuration for {repo_key}: {labels}\")\n",
    "        self.prediction_bug_label = labels.get(\"prediction_bug_label\", PREDICTION_BUG_LABEL)\n",
    "        self.prediction_enhancement_label = labels.get(\"prediction_enhancement_label\", PREDICTION_ENHANCEMENT_LABEL)\n",
    "        self.prediction_support_label = labels.get(\"prediction_support_label\", PREDICTION_SUPPORT_LABEL)\n",
    "\n",
    "    def append_label_config(self, new_data: dict, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        # Check if the config file exists\n",
    "        if os.path.exists(config_file):\n",
    "            # Load the existing configuration\n",
    "            with open(config_file, \"r\", encoding=\"utf-8-sig\") as file:\n",
    "                existing_data = toml.load(file)\n",
    "        else:\n",
    "            # If the file doesn\"t exist, start with an empty config\n",
    "            existing_data = {}\n",
    "\n",
    "        # Merge the new data with the existing data\n",
    "        existing_data.update(new_data)\n",
    "\n",
    "        # Write the updated configuration to the file\n",
    "        with open(config_file, \"w\", encoding=\"utf-8-sig\") as output_file:\n",
    "            toml.dump(existing_data, output_file)\n",
    "\n",
    "        logger.info(f\"Updated label configuration saved to {config_file}\")\n",
    "\n",
    "    def save_label_config(self, data: dict = {}, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "        # Save the default label configuration to the TOML file\n",
    "        with open(config_file, \"w\") as output_file:\n",
    "            toml.dump(data, output_file)\n",
    "\n",
    "        logger.info(f\"Label configuration saved to {config_file}\")\n",
    "    \n",
    "    def add_new_repo_to_label_config(self, config_file: str = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)):\n",
    "\n",
    "        # Fetch labels from the connected repository or use default labels if no repo is connected\n",
    "        if not self.repo:\n",
    "            logger.warning(\"No repository connected. Adding default label configuration.\")\n",
    "            repo = \"user_name/repo_name\"\n",
    "            bug_labels = [BUG, \"defect\"]\n",
    "            enhancement_labels = [ENHANCEMENT, \"feature\"]\n",
    "            support_labels = [SUPPORT, \"help\", \"question\"]\n",
    "        else:\n",
    "            repo = f\"{self.user_name}/{self.repository_name}\"\n",
    "            # Fetch labels from the connected repository\n",
    "            labels = self.repo.get_labels()\n",
    "            # Extract label names and descriptions\n",
    "            labels = [(label.name, label.description) for label in labels]\n",
    "            # Set descriptions to \"\" if they are None\n",
    "            labels = [(label_name, label_desc if label_desc else \"\") for label_name, label_desc in labels]\n",
    "\n",
    "            # Filter bug-related labels\n",
    "            bug_labels = [\n",
    "                label_name for label_name, label_desc in labels\n",
    "                if BUG.lower() in label_name.lower() or \"defect\" in label_name.lower() or BUG.lower() in label_desc.lower() or \"defect\" in label_desc.lower()\n",
    "            ]\n",
    "\n",
    "            # Filter enhancement-related labels\n",
    "            enhancement_labels = [\n",
    "                label_name for label_name, label_desc in labels\n",
    "                if ENHANCEMENT.lower() in label_name.lower() or \"feature\" in label_name.lower() or ENHANCEMENT.lower() in label_desc.lower() or \"feature\" in label_desc.lower()\n",
    "            ]\n",
    "\n",
    "            # Filter support/question-related labels\n",
    "            support_labels = [\n",
    "                label_name for label_name, label_desc in labels\n",
    "                if SUPPORT in label_name.lower() or \"question\" in label_name.lower() or \"help\" in label_name.lower() or SUPPORT.lower() in label_desc.lower() or \"question\" in label_desc.lower() or \"help\" in label_desc.lower()\n",
    "            ]\n",
    "\n",
    "        # Create the label configuration for this repository\n",
    "        new_repo_config = {\n",
    "            repo: {\n",
    "                BUG: bug_labels,\n",
    "                ENHANCEMENT: enhancement_labels,\n",
    "                SUPPORT: support_labels\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Add new configuration to the file or create it if it doesn\"t exist\n",
    "        self.append_label_config(new_repo_config, config_file)\n",
    "\n",
    "    # Save all data to a .json file\n",
    "    def save_data(self, partial: bool = False, repo_data_file: str = \"\"): # Please note that of partial is set to True, repo_data_file gets overwritten \n",
    "\n",
    "        dirname = \"\"\n",
    "        if not repo_data_file:\n",
    "            dirname = os.path.dirname(self.repo_data_file_path) or os.path.join(PROJECT_FOLDER, DATA_FOLDER) # Set to dirname of repo data file path if given and if not set to default dirname\n",
    "\n",
    "            if self.repo_data_file_path:\n",
    "                logger.debug(f\"Setting repo_data_file to self.repo_data_file_path: {self.repo_data_file_path} for saving\")\n",
    "                repo_data_file = self.repo_data_file_path\n",
    "            elif self.repo_data_file_name:\n",
    "                logger.debug(f\"Setting repo_data_file to self.repo_data_file_name: {self.repo_data_file_name} for saving\")\n",
    "                repo_data_file = self.repo_data_file_name\n",
    "            else: # if neither are given set to default value\n",
    "                repo_data_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}.json\")\n",
    "                logger.debug(f\"Setting repo_data_file to default: {repo_data_file} for saving\")\n",
    "        else:\n",
    "            dirname = os.path.dirname(repo_data_file)\n",
    "\n",
    "        if(dirname and (not os.path.exists(dirname))): os.makedirs(dirname)\n",
    "\n",
    "        if partial:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            repo_data_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}_partial_{timestamp}.json\")\n",
    "        \n",
    "        data = {\n",
    "            \"issues\": self.issues_data,\n",
    "            \"pull_requests\": self.pull_requests_data,\n",
    "            \"issue_pr_map\": self.issue_pr_map\n",
    "        }\n",
    "        \n",
    "        with open(repo_data_file, \"w\", encoding=\"utf-8-sig\") as output_file:\n",
    "            json.dump(data, output_file, indent=4)\n",
    "        logger.info(f\"Data saved to {repo_data_file}\")\n",
    "\n",
    "    # handle rate limit exceed exceptions\n",
    "    def rate_limit_exceed_handling(self):\n",
    "        logger.info(\"Rate limit exceeded again. Saving partial data...\")\n",
    "        self.save_data(partial=True)\n",
    "        logger.info(\"Partial data saved. Waiting until reset...\")\n",
    "        reset_time = self.github_api.rate_limiting_resettime\n",
    "        sleep_time = max(0, reset_time - time.time() + 10)  # Adding a buffer of 10 seconds\n",
    "        logger.info(f\"Sleeping for {sleep_time:.0f} seconds\")\n",
    "        time.sleep(sleep_time)\n",
    "        # Retry after sleep\n",
    "        logger.info(\"\\nRetrying...\")\n",
    "\n",
    "    # Map issues to PRs and save the data to issue_pr_map\n",
    "    def map_issues_to_prs(self): # TODO: implement support for issues being a dict instead of a list! BUG\n",
    "        for pr in self.pull_requests_data:\n",
    "            if pr[\"title\"]:\n",
    "                issue_numbers = re.findall(r\"#(\\d+)\", pr[\"title\"])\n",
    "                for issue_number in issue_numbers:\n",
    "                    if issue_number not in self.issue_pr_map: # Create a new list if not present\n",
    "                        self.issue_pr_map[issue_number] = []\n",
    "                    self.issue_pr_map[issue_number].append(pr[\"number\"])\n",
    "            if pr[\"description\"]:\n",
    "                issue_numbers = re.findall(r\"#(\\d+)\", pr[\"description\"])\n",
    "                for issue_number in issue_numbers:\n",
    "                    if issue_number not in self.issue_pr_map: # Create a new list if not present\n",
    "                        self.issue_pr_map[issue_number] = []\n",
    "                    self.issue_pr_map[issue_number].append(pr[\"number\"])\n",
    "\n",
    "    def check_rate_limit(self):\n",
    "        return self.repo.get_rate_limit()\n",
    "\n",
    "    # Fetch all issues\n",
    "    def fetch_issues(self):\n",
    "        # latest_issue = max(issue[\"number\"] for issue in self.issues_data.values())\n",
    "        latest_issue = 0\n",
    "        if len(self.issues_data) < 1: pass # No issue data found\n",
    "        elif isinstance(self.issues_data, dict): # Dict of dicts\n",
    "            latest_issue = max(issue[NUMBER] for issue in self.issues_data.values())\n",
    "        elif isinstance(self.issues_data, list): # List of dicts\n",
    "            latest_issue = max(issue[NUMBER] for issue in self.issues_data)\n",
    "        else:\n",
    "            raise TypeError(\"self.issues_data must be either a dict or a list.\")\n",
    "\n",
    "        try:\n",
    "            # issues = self.repo.get_issues(state=\"all\", sort=\"created\", direction=\"asc\", since=self.issues_last_fetch) # TODO: if since is used here, the skipping issues part will break fetching new issues\n",
    "            # fetch_time = datetime.now()\n",
    "            issues = self.repo.get_issues(state=\"all\", sort=\"created\", direction=\"asc\")\n",
    "            logger.info(f\"Amount of issues to fetch: {len(list(issues))}\")\n",
    "        except RateLimitExceededException as e:\n",
    "            logger.info(f\"\\n{e}\\n\")\n",
    "            self.rate_limit_exceed_handling()\n",
    "        \n",
    "        try:\n",
    "            logger.debug(len(self.issues_data))\n",
    "            for i, issue in enumerate(issues):\n",
    "                # if(i < len(self.issues_data)): continue # TODO: unnecessary with since... choose either! If this is used, please check if updates of issues affect this\n",
    "                if issue.number < latest_issue: continue\n",
    "                \n",
    "                # # self.issues_data[issue.number] = { # dict version\n",
    "                # self.issues_data.append({ # list version\n",
    "                #     \"number\": issue.number, # redundant for dict version\n",
    "                #     \"title\": issue.title,\n",
    "                #     \"state\": issue.state, # calculateable, but still required as we didn\"t implement the logic to update the state of an issue using closed_at\n",
    "                #     \"created_at\": issue.created_at.isoformat(),\n",
    "                #     \"closed_at\": issue.closed_at.isoformat() if issue.closed_at else None,\n",
    "                #     \"user\": issue.user.login, # optional\n",
    "                #     \"comments\": issue.comments, # optional\n",
    "                #     \"description\": issue.body,\n",
    "                #     \"description_length\": len(issue.body) if issue.body else 0, # calculateable\n",
    "                #     \"labels\": [label.name for label in issue.labels],\n",
    "                #     \"predicted_label\": None,\n",
    "                #     \"author_association\": issue.raw_data.get(\"author_association\", \"NONE\")\n",
    "                # # } # dict version\n",
    "                # }) # list version\n",
    "\n",
    "                # self.issues_data[issue.number] = { # dict version\n",
    "                self.issues_data.append({ # list version\n",
    "                    NUMBER: issue.number, # redundant for dict version\n",
    "                    TITLE: issue.title,\n",
    "                    \"state\": issue.state, # calculateable, but still required as we didn\"t implement the logic to update the state of an issue using closed_at\n",
    "                    \"created_at\": issue.created_at.isoformat(),\n",
    "                    \"closed_at\": issue.closed_at.isoformat() if issue.closed_at else None,\n",
    "                    \"user\": issue.user.login, # optional\n",
    "                    \"comments\": issue.comments, # optional\n",
    "                    BODY: issue.body,\n",
    "                    \"description_length\": len(issue.body) if issue.body else 0, # calculateable\n",
    "                    LABEL: [label.name for label in issue.labels],\n",
    "                    PREDICTED_LABEL: None,\n",
    "                    AUTHOR: issue.raw_data.get(\"author_association\", \"NONE\")\n",
    "                # } # dict version\n",
    "                }) # list version\n",
    "\n",
    "                if(i % 100 == 0): logger.debug(i)\n",
    "            # self.issues_last_fetch = fetch_time\n",
    "            self.save_data(False)\n",
    "        except RateLimitExceededException as e:\n",
    "            logger.debug(f\"\\n{e}\\n\")\n",
    "            self.save_data(True)\n",
    "            self.rate_limit_exceed_handling()\n",
    "            self.fetch_issues()\n",
    "        except Exception as e:\n",
    "            self.save_data(True)\n",
    "            logger.error(f\"\\n{e}\\n\")\n",
    "            self.fetch_issues()\n",
    "\n",
    "    # Fetch all pull requests\n",
    "    def fetch_pull_requests(self):\n",
    "        latest_pr = 0\n",
    "        if len(self.pull_requests_data) < 1: pass # No issue data found\n",
    "        elif isinstance(self.pull_requests_data, dict): # Dict of dicts\n",
    "            latest_pr = max(pr[NUMBER] for pr in self.pull_requests_data.values())\n",
    "        elif isinstance(self.pull_requests_data, list): # List of dicts\n",
    "            latest_pr = max(pr[NUMBER] for pr in self.pull_requests_data)\n",
    "        else:\n",
    "            raise TypeError(\"self.issues_data must be either a dict or a list.\")\n",
    "            \n",
    "        try:\n",
    "            prs = self.repo.get_pulls(state=\"all\", sort=\"created\", direction=\"asc\")\n",
    "        except RateLimitExceededException as e:\n",
    "            logger.info(f\"\\n{e}\\n\")\n",
    "            self.rate_limit_exceed_handling()\n",
    "\n",
    "        try:\n",
    "            for i, pr in enumerate(prs):\n",
    "                # if(i < len(self.pull_requests_data)): continue\n",
    "                if i < latest_pr: continue\n",
    "                \n",
    "                pr_files = pr.get_files()\n",
    "                files_changed = [{\n",
    "                    \"filename\": pr_file.filename,\n",
    "                    \"additions\": pr_file.additions,\n",
    "                    \"deletions\": pr_file.deletions\n",
    "                } for pr_file in pr_files]\n",
    "                \n",
    "                pr_commits = pr.get_commits()\n",
    "                commit_data = [{\n",
    "                    \"sha\": commit.sha, # optional\n",
    "                    \"author\": commit.commit.author.name, # optional\n",
    "                    \"date\": commit.commit.author.date.isoformat(),\n",
    "                    \"message\": commit.commit.message\n",
    "                } for commit in pr_commits]\n",
    "\n",
    "                # self.pull_requests_data[pr.number] = { # dict version\n",
    "                self.pull_requests_data.append({ # list version\n",
    "                    \"number\": pr.number, # redundant\n",
    "                    \"title\": pr.title,\n",
    "                    \"state\": pr.state, # calculateable, but still required as we didn\"t implement the logic to update the state of a pull request using closed_at\n",
    "                    \"created_at\": pr.created_at.isoformat(),\n",
    "                    \"merged_at\": pr.merged_at.isoformat() if pr.merged_at else None,\n",
    "                    \"closed_at\": pr.closed_at.isoformat() if pr.closed_at else None,\n",
    "                    \"user\": pr.user.login, # optional\n",
    "                    \"comments\": pr.comments, # optional\n",
    "                    \"review_comments\": pr.review_comments, # optional\n",
    "                    \"description\": pr.body,\n",
    "                    \"description_length\": len(pr.body) if pr.body else 0, # calculateable\n",
    "                    \"additions\": pr.additions,\n",
    "                    \"deletions\": pr.deletions,\n",
    "                    \"changed_files\": pr.changed_files,\n",
    "                    \"files\": files_changed,\n",
    "                    \"commits\": commit_data,\n",
    "                    \"labels\": [label.name for label in pr.labels],\n",
    "                    \"mergeable_state\": pr.mergeable_state\n",
    "                # } # dict version\n",
    "                }) # list version\n",
    "            self.save_data(False)\n",
    "        except RateLimitExceededException as e:\n",
    "            logger.debug(f\"\\n{e}\\n\")\n",
    "            self.save_data(True)\n",
    "            self.rate_limit_exceed_handling()\n",
    "            self.fetch_pull_requests()  \n",
    "        except Exception as e:\n",
    "            logger.error(f\"\\n{e}\\n\")\n",
    "            self.save_data(True)\n",
    "            self.fetch_pull_requests()\n",
    "\n",
    "    def issue_exists(self, issue_number: int) -> bool:\n",
    "        return issue_number in self.issues_data.keys()\n",
    "\n",
    "    def pr_exists(self, pr_number: int) -> bool:\n",
    "        return pr_number in self.pull_requests_data.keys()\n",
    "\n",
    "    def get_issue(self, issue_number: int) -> dict:\n",
    "        return self.issues_data[issue_number]\n",
    "    \n",
    "    def get_pr(self, pr_number: int) -> dict:\n",
    "        return self.pull_requests_data[pr_number]\n",
    "    \n",
    "    def get_issues(self) -> dict:\n",
    "        return self.issues_data\n",
    "    \n",
    "    def get_prs(self) -> dict:\n",
    "        return self.pull_requests_data\n",
    "\n",
    "    def load_results_dataframe(self, df=None, csv_path=None):\n",
    "        if df is not None:\n",
    "            logger.info(\"Loading provided DataFrame as results.\")\n",
    "            self.results_df = df\n",
    "            \n",
    "        elif csv_path:\n",
    "            logger.info(f\"Loading results DataFrame from CSV: {csv_path}\")\n",
    "            self.results_df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either `csv_path` or `df` must be provided.\")\n",
    "\n",
    "    def update_issue(self, issue_number: int = None, predicted_label: str = \"\"):\n",
    "        try:\n",
    "            issue = self.repo.get_issue(number=issue_number)\n",
    "            # Get the current labels on the issue\n",
    "            current_labels = issue.get_labels()\n",
    "            \n",
    "            # Extract label names\n",
    "            current_label_names = [label.name for label in current_labels]\n",
    "\n",
    "            # TODO: TEST THIS - START\n",
    "            if predicted_label == BUG:\n",
    "                predicted_label = self.prediction_bug_label\n",
    "            elif predicted_label == ENHANCEMENT:\n",
    "                predicted_label = self.prediction_enhancement_label\n",
    "            elif predicted_label == SUPPORT:\n",
    "                predicted_label = self.prediction_support_label\n",
    "            else:\n",
    "                predicted_label = \"Unknown\"\n",
    "            # TODO: TEST THIS - END\n",
    "                \n",
    "            # Add new labels and ensure that there are no duplicates\n",
    "            new_labels = list(set(current_label_names + [predicted_label])) # TODO: change predicted label to the one specified for that class in the config!\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error when trying to fetch add add predicted label {predicted_label} for issue {issue_number}: {e} \")\n",
    "        \n",
    "        # Set the combined labels\n",
    "        try:\n",
    "            issue.set_labels(*new_labels)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error when trying to update labels of issue {issue_number} with label {predicted_label}: {e}\")\n",
    "            \n",
    "    def update_issues(self, df=None):\n",
    "        try:\n",
    "            permissions = self.repo.permissions\n",
    "            if not permissions.push:\n",
    "                raise PermissionError(\n",
    "                    \"The authenticated user does not have write permissions for this repository. Can't update issues.\"\n",
    "                )\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"Unable to verify repository permissions. Check the connection to GitHub.\")\n",
    "        \n",
    "        if self.results_df is None:\n",
    "            if df is not None:\n",
    "                self.results_df = df\n",
    "            else:\n",
    "                raise ValueError(\"No dataframe containing predictions found. Can't update issues.\")\n",
    "        \n",
    "        needed_columns = [NUMBER, PREDICTED_LABEL]\n",
    "\n",
    "        for col in needed_columns:\n",
    "            if col not in self.results_df.columns:\n",
    "                raise ValueError(f\"Results dataframe doesn't contain column {col}. Can't update issues.\")\n",
    "\n",
    "        results_df = self.results_df[needed_columns]        \n",
    "    \n",
    "        for _, row in results_df.iterrows():\n",
    "            self.update_issue(issue_number = row[NUMBER], predicted_label = row[PREDICTED_LABEL])\n",
    "\n",
    "    def save_predictions_to_issue_data(self, df=None): # TODO: fix: BUG: doesn't work, because issue_data is a list not a dictionary\n",
    "        if df is not None:\n",
    "            self.results_df = df\n",
    "            \n",
    "        if self.results_df is None:\n",
    "            raise ValueError(\"No dataframe containing predictions found. Can't update issues.\")\n",
    "        \n",
    "        needed_columns = [NUMBER, PREDICTED_LABEL]\n",
    "\n",
    "        for col in needed_columns:\n",
    "            if col not in self.results_df.columns:\n",
    "                raise ValueError(f\"Results dataframe doesn't contain column {col}. Can't update issues.\")\n",
    "\n",
    "        results_df = self.results_df[needed_columns]\n",
    "\n",
    "        for issue in self.issues_data:\n",
    "            matching_row = results_df[results_df[NUMBER] == issue[NUMBER]]\n",
    "            if not matching_row.empty:\n",
    "                issue[PREDICTED_LABEL] = matching_row.iloc[0][PREDICTED_LABEL]\n",
    "            \n",
    "        # for _, row in results_df.iterrows():\n",
    "        #     issue_number = str(row[NUMBER])\n",
    "        #     predicted_label = row[PREDICTED_LABEL]\n",
    "        #     if issue_number in self.issues_data:\n",
    "        #         self.issues_data[issue_number][\"predicted_label\"] = predicted_label\n",
    "        #     else:\n",
    "        #         logger.error(f\"No entry in data found for issue number \\\"{issue_number}\\\". Adding new entry to data.\")\n",
    "        #         self.issues_data[issue_number] = {\"predicted_label\": predicted_label}\n",
    "    \n",
    "        self.save_data(partial=True) # TODO: remove \"partial=True\" once testing is done!\n",
    "\n",
    "    def save_predictions_to_issue_data_and_update_issues(self, df=None): # TODO: fix: BUG: doesn't work, because issue_data is a list not a dictionary\n",
    "        # TODO: check in which order the 2 lines below should get updated\n",
    "        self.update_issues(df=df)\n",
    "        self.save_predictions_to_issue_data(df=df)\n",
    "        \n",
    "        # try:\n",
    "        #     permissions = self.repo.permissions\n",
    "        #     if not permissions.push:\n",
    "        #         raise PermissionError(\n",
    "        #             \"The authenticated user does not have write permissions for this repository. Can't update issues.\"\n",
    "        #         )\n",
    "        # except AttributeError:\n",
    "        #     raise RuntimeError(\"Unable to verify repository permissions. Check the connection to GitHub.\")\n",
    "        \n",
    "        # if self.results_df is None:\n",
    "        #     if df is not None:\n",
    "        #         self.results_df = df\n",
    "        #     else:\n",
    "        #         raise ValueError(\"No dataframe containing predictions found. Can't update issues.\")\n",
    "        \n",
    "        # needed_columns = [NUMBER, PREDICTED_LABEL]\n",
    "\n",
    "        # for col in needed_columns:\n",
    "        #     if col not in results_df.columns:\n",
    "        #         raise ValueError(f\"Results dataframe doesn't contain column {col}. Can't update issues.\")\n",
    "\n",
    "        # results_df = self.results_df[needed_columns]        \n",
    "    \n",
    "        # for _, row in results_df.iterrows():\n",
    "        #     try:\n",
    "        #         self.update_issue(issue_number = row[NUMBER], predicted_label = row[PREDICTED_LABEL])\n",
    "        #     except Exception as e:\n",
    "        #         self.save()\n",
    "        #         logger.error(f\"When trying to update issue {issue_number} with label {predicted_label} an exception occured: {e}\")\n",
    "        #         break # stop updating for now... TODO: keep it this way or continue trying for others? (also handle different exceptions differently?)\n",
    "\n",
    "        #     issue_number = str(row[NUMBER])\n",
    "        #     predicted_label = row[PREDICTED_LABEL]\n",
    "        #     if issue_number in self.issues_data:\n",
    "        #         self.issues_data[issue_number][\"predicted_label\"] = predicted_label\n",
    "        #     else:\n",
    "        #         logger.error(f\"No entry in data found for issue number \\\"{issue_number}\\\". Adding new entry to data.\")\n",
    "        #         self.issues_data[issue_number] = {\"predicted_label\": predicted_label}\n",
    "\n",
    "        # self.save()\n",
    "\n",
    "    \n",
    "    def check_full_availability(self, template_dict, data_dict):\n",
    "        result = {}\n",
    "        for key, value in template_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively check nested dictionaries\n",
    "                result[key] = all(\n",
    "                    self.check_full_availability(value, item.get(key, {}))\n",
    "                    for item in data_dict if isinstance(item, dict)\n",
    "                )\n",
    "            else:\n",
    "                # Handle the case where data_dict is a list of dictionaries\n",
    "                result[key] = all(\n",
    "                    key in item and item[key] is not None\n",
    "                    for item in data_dict if isinstance(item, dict)\n",
    "                )\n",
    "        return result\n",
    "\n",
    "    def check_partial_availability(self, template_dict, data_dict):\n",
    "        result = {}\n",
    "        for key, value in template_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                # Recursively check nested dictionaries\n",
    "                result[key] = any(\n",
    "                    self.check_partial_availability(value, item.get(key, {}))\n",
    "                    for item in data_dict if isinstance(item, dict)\n",
    "                )\n",
    "            else:\n",
    "                # Handle the case where data_dict is a list of dictionaries\n",
    "                result[key] = any(\n",
    "                    key in item and item[key] is not None\n",
    "                    for item in data_dict if isinstance(item, dict)\n",
    "                )\n",
    "        return result\n",
    "    \n",
    "    def check_availability(self):\n",
    "        # Full and partial availability checks\n",
    "        if(self.issues_data): self.fully_available_issue_data = self.check_full_availability(self.fully_available_issue_data, self.issues_data)\n",
    "        if(self.pull_requests_data): self.fully_available_pr_data = self.check_full_availability(self.fully_available_pr_data, self.pull_requests_data)\n",
    "\n",
    "        if(self.issues_data): self.partially_available_issue_data = self.check_partial_availability(self.partially_available_issue_data, self.issues_data)\n",
    "        if(self.pull_requests_data): self.partially_available_pr_data = self.check_partial_availability(self.partially_available_pr_data, self.pull_requests_data)\n",
    "\n",
    "        self.availability_checked = True\n",
    "\n",
    "    def check_function_executability(function_requirements):\n",
    "\n",
    "        if not self.availability_checked: self.check_availability()\n",
    "            \n",
    "        executable_functions = {}\n",
    "\n",
    "        data_availability_dicts = {\n",
    "            ISSUES: {\n",
    "                \"partially_available\": self.partially_available_issue_data,\n",
    "                \"fully_available\": self.fully_available_issue_data\n",
    "            },\n",
    "            PRS: {\n",
    "                \"partially_available\": self.partially_available_pr_data,\n",
    "                \"fully_available\": self.fully_available_pr_data\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        for func, data_requirements in function_requirements.items():\n",
    "            is_executable = True  # Assume executable unless a requirement fails\n",
    "    \n",
    "            for data_type, requirements in data_requirements.items():\n",
    "                fully_available = data_availability_dicts[data_type][\"fully_available\"]\n",
    "                partially_available = data_availability_dicts[data_type][\"partially_available\"]\n",
    "    \n",
    "                for key, required_availability in requirements.items():\n",
    "                    if required_availability == \"full\":\n",
    "                        if not fully_available.get(key, False):\n",
    "                            is_executable = False\n",
    "                            break\n",
    "                    elif required_availability == \"partial\":\n",
    "                        if not (fully_available.get(key, False) or partially_available.get(key, False)):\n",
    "                            is_executable = False\n",
    "                            break\n",
    "    \n",
    "                if not is_executable:\n",
    "                    break  # Stop checking other data types if one fails\n",
    "    \n",
    "            executable_functions[func] = \"executable\" if is_executable else \"not_executable\"\n",
    "    \n",
    "        return executable_functions\n",
    "    \n",
    "    def get_fully_available_issue_data(self):\n",
    "        return self.fully_available_issue_data\n",
    "    \n",
    "    def get_fully_available_pr_data(self):\n",
    "        return self.fully_available_pr_data\n",
    "    \n",
    "    def get_partially_available_issue_data(self):\n",
    "        return self.partially_available_issue_data\n",
    "    \n",
    "    def get_partially_available_pr_data(self):\n",
    "        return self.partially_available_pr_data\n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return {BUG: self.bug_labels, ENHANCEMENT: self.enhancement_labels, SUPPORT: self.support_labels}\n",
    "\n",
    "    def get_prediction_labels(self):\n",
    "        return {BUG: self.prediction_bug_label, ENHANCEMENT: self.prediction_enhancement_label, SUPPORT: self.prediction_support_label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RepoHandler Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_NAME = \"vaadin\"\n",
    "# REPO_NAME = \"flow\"\n",
    "# repoHandler = RepoDataHandler(user_name=USER_NAME, repository_name=REPO_NAME, token=GITHUB_PAT)\n",
    "# repoHandler.load_repo_data_config()\n",
    "# repoHandler.load_data()\n",
    "# if FETCH_NEW_ISSUES: repoHandler.fetch_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestRepoDataHandler(unittest.TestCase):\n",
    "\n",
    "#     def setUp(self):\n",
    "#         # Initialize the RepoDataHandler with mocked data\n",
    "#         self.handler = RepoDataHandler(\n",
    "#             user_name=\"grafana\",\n",
    "#             repository_name=\"grafana\"\n",
    "#         )\n",
    "#         self.mock_repo = MagicMock()\n",
    "#         self.handler.repo = self.mock_repo\n",
    "#         self.handler.github_api = MagicMock()\n",
    "\n",
    "#         # Set the path for the actual JSON file\n",
    "#         self.repo_data_json_file = os.path.join(PROJECT_FOLDER, DATA_FOLDER, \"grafana_grafana_repo_data.json\")\n",
    "\n",
    "#     def test_load_data_from_file(self):\n",
    "#         # Load the actual data from the specified file\n",
    "#         with open(self.repo_data_json_file, \"r\", encoding=\"utf-8-sig\") as input_file:\n",
    "#             data = json.load(input_file)\n",
    "#             self.handler.issues_data = data[\"issues\"]\n",
    "#             self.handler.pull_requests_data = data[\"pull_requests\"]\n",
    "#             self.handler.issue_pr_map = data[\"issue_pr_map\"]\n",
    "\n",
    "#         # Assert that data is loaded correctly (use appropriate checks)\n",
    "#         self.assertIn(\"issues\", data)\n",
    "#         self.assertIn(\"pull_requests\", data)\n",
    "#         self.assertIn(\"issue_pr_map\", data)\n",
    "#         self.assertGreater(len(self.handler.issues_data), 0)\n",
    "#         self.assertGreater(len(self.handler.pull_requests_data), 0)\n",
    "#         self.assertGreater(len(self.handler.issue_pr_map), 0)\n",
    "\n",
    "#     def test_check_availability(self):\n",
    "#         # Assuming data is loaded, we now check full and partial availability\n",
    "#         self.test_load_data_from_file()\n",
    "\n",
    "#         # Run the availability check\n",
    "#         self.handler.check_availability()\n",
    "\n",
    "#         # Check that the availability structures have been updated\n",
    "#         for value in self.handler.fully_available_issue_data.values():\n",
    "#             self.assertIsInstance(value, bool)\n",
    "\n",
    "#         for value in self.handler.fully_available_pr_data.values():\n",
    "#             self.assertIsInstance(value, bool)\n",
    "\n",
    "#         # Check that the availability structures have been updated\n",
    "#         for value in self.handler.partially_available_issue_data.values():\n",
    "#             self.assertIsInstance(value, bool)\n",
    "\n",
    "#         for value in self.handler.partially_available_pr_data.values():\n",
    "#             self.assertIsInstance(value, bool)\n",
    "\n",
    "#     def test_load_toml_config(self):\n",
    "#         # Path to TOML config file (mocked in the test)\n",
    "#         # config_file = os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, LABEL_CONFIG_FILE_NAME)\n",
    "\n",
    "#         # Mock TOML data that simulates the contents of the file\n",
    "#         toml_data = \"\"\"\n",
    "#         [\"vaadin/flow\"]\n",
    "#         bug_labels = [\"bug\", \"BFP\"]\n",
    "#         enhancement_labels = [\"enhancement\", \"feature request\"]\n",
    "#         support_labels = [\"documentation\", \"question\", \"help wanted\"]\n",
    "\n",
    "#         [\"grafana/grafana\"]\n",
    "#         bug_labels = [\"type/bug\"]\n",
    "#         enhancement_labels = [\"type/feature-request\", \"kind/enhancement\", \"discussion/consideration\"]\n",
    "#         support_labels = [\"type/docs\", \"type/question\", \"bot/question\"]\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Mock open to simulate reading the TOML file\n",
    "#         with patch(\"builtins.open\", unittest.mock.mock_open(read_data=toml_data)):\n",
    "#             self.handler.load_label_config()\n",
    "\n",
    "#         logger.debug(self.handler.bug_labels)\n",
    "#         logger.debug(self.handler.enhancement_labels)\n",
    "#         logger.debug(self.handler.support_labels)\n",
    "\n",
    "#         # Assertions to verify that the label configuration is loaded correctly\n",
    "#         self.assertEqual(self.handler.bug_labels, [\"type/bug\"])\n",
    "#         self.assertEqual(self.handler.enhancement_labels, [\"type/feature-request\", \"kind/enhancement\", \"discussion/consideration\"])\n",
    "#         self.assertEqual(self.handler.support_labels, [\"type/docs\", \"type/question\", \"bot/question\"])\n",
    "\n",
    "\n",
    "# # Run the tests directly in the Jupyter notebook\n",
    "# unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestRepoDataHandler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatIss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, \n",
    "                 data: dict = {}, \n",
    "                 label_names: dict = {}, \n",
    "                 user_name: str = None,\n",
    "                 repository_name: str = None,\n",
    "                 evaluation_mode: bool = EVALUATION, \n",
    "                 test_mode: bool = TEST, \n",
    "                 validation_split_percentage: float = VALIDATION_SPLIT_PERCENTAGE, \n",
    "                 random_state: int = RANDOM_STATE):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor for handling preprocessing tasks.\n",
    "\n",
    "        :param data: Dictionary containing raw features and labels for each example.\n",
    "        :param label_names: Dictionary mapping label types to lists of labels (e.g., {\"bug\": [...], \"enhancement\": [...], \"support\": [...]}).\n",
    "        :param user_name: GitHub user name (for constructing URLs).\n",
    "        :param repository_name: GitHub repository name (for constructing URLs).\n",
    "        :param evaluation_mode: If True, data will be split into training and evaluation sets.\n",
    "        :param test_mode: If True, rows without labels will be saved for testing.\n",
    "        :param validation_split_percentage: Proportion of data used for evaluation (default is 20%).\n",
    "        :param random_state: Seed for reproducibility of data splitting.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.label_names = label_names\n",
    "        self.evaluation_mode = evaluation_mode\n",
    "        self.test_mode = test_mode\n",
    "        self.validation_split_percentage = validation_split_percentage\n",
    "        self.random_state = random_state\n",
    "        self.user_name = user_name\n",
    "        self.repository_name = repository_name\n",
    "        self.train_csv = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}_train_clean_concat_{MAX_TITLE_LENGTH + MAX_BODY_LENGTH}.csv\")\n",
    "        self.eval_csv = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}_eval_clean_concat_{MAX_TITLE_LENGTH + MAX_BODY_LENGTH}.csv\")\n",
    "        self.test_csv = os.path.join(PROJECT_FOLDER, DATA_FOLDER, f\"{self.user_name}_{self.repository_name}_test_clean_concat_{MAX_TITLE_LENGTH + MAX_BODY_LENGTH}.csv\")\n",
    "\n",
    "    def filter_and_select_label(self, label_list):\n",
    "        \"\"\"\n",
    "        Filters and selects the first valid label from the provided list.\n",
    "\n",
    "        :param label_list: List of labels for a specific issue or pull request.\n",
    "        :return: The first matching label category (e.g., \"bug\", \"enhancement\") or None if no match is found.\n",
    "        \"\"\"\n",
    "        # return None if predicted_label isn't None? - NOT HERE\n",
    "        for label in label_list:\n",
    "            for key in self.label_names:\n",
    "                if label in self.label_names[key]:\n",
    "                    return key\n",
    "        return None\n",
    "\n",
    "    def create_df(self):\n",
    "        \"\"\"\n",
    "        Creates a DataFrame from the raw data and applies necessary filtering and transformations.\n",
    "        - Filters for necessary columns.\n",
    "        - Replaces the labels with the first valid label from the list of labels.\n",
    "        - Adds the GitHub repository URL.\n",
    "        - Adjusts the time format by adding the \"Z\" suffix.\n",
    "        \"\"\"\n",
    "        self.df = pd.DataFrame(self.data)\n",
    "\n",
    "        # logger.debug(self.df.head())\n",
    "        logger.debug(f\"self.df.columns: {self.df.columns}\")\n",
    "\n",
    "        self.df[REPO] = f\"https://api.github.com/repos/{self.user_name}/{self.repository_name}\"\n",
    "\n",
    "        # logger.debug(self.df.head())\n",
    "        logger.debug(f\"self.df.columns: {self.df.columns}\")\n",
    "\n",
    "        # self.df[\"number\"] = [f\"https://api.github.com/repos/{self.user_name}/{self.repository_name}/issues/{issue_number}\" for issue_number in self.df[\"number\"]] # {self.user_name}/{self.repository_name}/{issue_number} would achieve the exact same\n",
    "        # self.df.rename(columns={\"number\": URL}, inplace=True)\n",
    "        # TODO: test line below instead of the 2 above\n",
    "        self.df[URL] = [f\"https://api.github.com/repos/{self.user_name}/{self.repository_name}/issues/{issue_number}\" for issue_number in self.df[\"number\"]] # {self.user_name}/{self.repository_name}/{issue_number} would achieve the exact same\n",
    "\n",
    "        # Ensure necessary columns are present in the data\n",
    "        if not all(col in self.df.columns for col in NECESSARY_COLUMNS):\n",
    "            logger.debug(f\"self.df.columns: {self.df.columns}\")\n",
    "            raise ValueError(f\"Missing necessary columns. Required: {NECESSARY_COLUMNS} ({REPO} is calculated from user_name and repository_name and {URL} from issue number during runtime).\")\n",
    "\n",
    "        if(not \"number\" in self.df.columns): self.df[\"number\"] = self.df[URL]\n",
    "        self.df = self.df[USEFUL_COLUMNS]\n",
    "        self.df[LABEL] = self.df[LABEL].apply(self.filter_and_select_label)\n",
    "        self.df[TIME] = self.df[TIME] + \"Z\"\n",
    "\n",
    "    def filter_df(self):\n",
    "        all_labels_to_filter = [BUG, ENHANCEMENT, SUPPORT] # self.label_names[BUG] + self.label_names[ENHANCEMENT] + self.label_names[SUPPORT] # TODO: test and verify which one is required at this point. (since label mapping has been done priorily it should be the first one(?))\n",
    "        logger.debug(f\"all labels to filter: {all_labels_to_filter}\")\n",
    "        \n",
    "        # Filter out rows where the 'labels' column contains any of the labels in all_labels_to_filter\n",
    "        logger.debug(\"self.df[LABEL].value_counts():\")\n",
    "        logger.debug(self.df[LABEL].value_counts())\n",
    "        logger.debug(f\"self.df[{PREDICTED_LABEL}].value_counts(dropna=False):\")\n",
    "        logger.debug(self.df[PREDICTED_LABEL].value_counts(dropna=False))\n",
    "        if(self.test_mode): # TODO: add that issues, which already have already a prediciton, get ignored here\n",
    "            self.test = self.df[~self.df[LABEL].isin(all_labels_to_filter)]\n",
    "            logger.debug(\"Value counts for test data frame BEFORE removing all the ones that already have a prediction:\")\n",
    "            logger.debug(self.test[PREDICTED_LABEL].value_counts(dropna=False))\n",
    "            self.test = self.test[self.test[PREDICTED_LABEL].isnull()] # TODO: test\n",
    "            logger.debug(\"Value counts for test data frame AFTER removing all the ones that already have a prediction:\")\n",
    "            logger.debug(self.test[PREDICTED_LABEL].value_counts(dropna=False))\n",
    "            logger.debug(\"self.test.value_counts():\")\n",
    "            logger.debug(self.df[LABEL].value_counts())\n",
    "        self.df = self.df[self.df[LABEL].isin(all_labels_to_filter)]\n",
    "        logger.debug(\"self.df.value_counts():\")\n",
    "        logger.debug(self.df[LABEL].value_counts())\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"\n",
    "        Splits data into training, evaluation, and test sets based on the provided mode.\n",
    "        - Test data contains rows without labels.\n",
    "        - Data is split into training and evaluation sets if evaluation_mode is True.\n",
    "        \"\"\"\n",
    "        if(not self.test_mode): self.test = None\n",
    "        \n",
    "        # self.test = None\n",
    "        # if self.test_mode:\n",
    "            # self.test = self.df[self.df[LABEL].isna()]\n",
    "            # self.filter_df()\n",
    "\n",
    "        # self.df.dropna(subset=[LABEL], inplace=True)\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            self.train, self.eval = train_test_split(self.df, test_size=self.validation_split_percentage, random_state=self.random_state)\n",
    "        else:\n",
    "            self.train = self.df\n",
    "            self.eval = None\n",
    "\n",
    "    def deduplicate_data(self):\n",
    "        \"\"\"\n",
    "        Deduplicates the training data based on the \"url\" column (e.g., issue number).\n",
    "        - Ensures all text columns are converted to string type after deduplication.\n",
    "        \"\"\"\n",
    "        self.dedup_train = self.train.sort_values(URL).drop_duplicates(subset=[URL]).copy()\n",
    "        logger.debug(f\"Number of dropped duplicate issues: {self.train.shape[0] - self.dedup_train.shape[0]}\")\n",
    "\n",
    "        # Ensure columns are strings\n",
    "        for col in [TITLE, BODY, AUTHOR, TIME, REPO]:\n",
    "            self.dedup_train[col] = self.dedup_train[col].astype(str)\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            for col in [TITLE, BODY, AUTHOR, TIME, REPO]:\n",
    "                self.eval[col] = self.eval[col].astype(str)\n",
    "\n",
    "        if self.test_mode:\n",
    "            for col in [TITLE, BODY, AUTHOR, TIME, REPO]:\n",
    "                self.test[col] = self.test[col].astype(str)\n",
    "\n",
    "    def normalize_text(self):\n",
    "        \"\"\"\n",
    "        Normalizes text data in the \"title\" and \"body\" fields.\n",
    "        - Replaces functions and issue numbers with placeholders.\n",
    "        - Converts text to lowercase.\n",
    "        \"\"\"\n",
    "        # self.dedup_train[body] = self.dedup_train[body].apply(lambda x: FUNCTION_REGEX.sub(\" function \", x))\n",
    "        # self.dedup_train[title] = self.dedup_train[title].apply(lambda x: ISSUE_REGEX.sub(\" issue \", x))\n",
    "        # self.dedup_train[title] = self.dedup_train[title].str.lower()\n",
    "        # self.dedup_train[body] = self.dedup_train[body].str.lower()\n",
    "\n",
    "        # if self.evaluation_mode:\n",
    "        #     self.eval[body] = self.eval[body].apply(lambda x: FUNCTION_REGEX.sub(\" function \", x))\n",
    "        #     self.eval[title] = self.eval[title].apply(lambda x: ISSUE_REGEX.sub(\" issue \", x))\n",
    "        #     self.eval[title] = self.eval[title].str.lower()\n",
    "        #     self.eval[body] = self.eval[body].str.lower()\n",
    "\n",
    "        # if self.test_mode:\n",
    "        #     self.test[body] = self.test[body].apply(lambda x: FUNCTION_REGEX.sub(\" function \", x))\n",
    "        #     self.test[title] = self.test[title].apply(lambda x: ISSUE_REGEX.sub(\" issue \", x))\n",
    "        #     self.test[title] = self.test[title].str.lower()\n",
    "        #     self.test[body] = self.test[body].str.lower()\n",
    "\n",
    "        logger.debug(\"Replacing functions...\")\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x:FUNCTION_REGEX.sub(\" function \",x))\n",
    "        \n",
    "        if(self.evaluation_mode): self.eval[BODY] = self.eval[BODY].apply(lambda x:FUNCTION_REGEX.sub(\" function \",x))\n",
    "        if(self.test_mode): self.test[BODY] = self.test[BODY].apply(lambda x:FUNCTION_REGEX.sub(\" function \",x))\n",
    "\n",
    "        logger.debug(\"Replacing issue numbers...\")\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "        \n",
    "        if(self.evaluation_mode):\n",
    "            self.eval[TITLE] = self.eval[TITLE].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "            self.eval[BODY] = self.eval[BODY].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "\n",
    "        if(self.test_mode):\n",
    "            self.test[TITLE] = self.test[TITLE].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "            self.test[BODY] = self.test[BODY].apply(lambda x:ISSUE_REGEX.sub(\" issue \",x))\n",
    "\n",
    "        logger.debug(\"Converting to lower case...\")\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].str.lower()\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].str.lower()\n",
    "        \n",
    "        if(self.evaluation_mode):\n",
    "            self.eval[TITLE] = self.eval[TITLE].str.lower()\n",
    "            self.eval[BODY] = self.eval[BODY].str.lower()\n",
    "\n",
    "        if(self.test_mode):\n",
    "            self.test[TITLE] = self.test[TITLE].str.lower()\n",
    "            self.test[BODY] = self.test[BODY].str.lower()\n",
    "\n",
    "    def remove_extra_info(self):\n",
    "        \"\"\"\n",
    "        Removes extraneous information from the text data.\n",
    "        - Removes punctuation, non-ASCII characters, and extra spaces from the text fields.\n",
    "        \"\"\"\n",
    "        # Remove punctuation\n",
    "        logger.debug(\"Removing punctuation...\")\n",
    "        replace_string = \" \" * len(PUNCTUATIONS)\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            self.eval[TITLE] = self.eval[TITLE].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "            self.eval[BODY] = self.eval[BODY].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "\n",
    "        if self.test_mode:\n",
    "            self.test[TITLE] = self.test[TITLE].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "            self.test[BODY] = self.test[BODY].str.translate(str.maketrans(PUNCTUATIONS, replace_string))\n",
    "\n",
    "        # Remove non-ASCII characters and normalize\n",
    "        logger.debug(\"Removing non-ASCII characters...\")\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            self.eval[TITLE] = self.eval[TITLE].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "            self.eval[TITLE] = self.eval[TITLE].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "            self.eval[BODY] = self.eval[BODY].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "            self.eval[BODY] = self.eval[BODY].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "\n",
    "        if self.test_mode:\n",
    "            self.test[TITLE] = self.test[TITLE].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "            self.test[TITLE] = self.test[TITLE].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "            self.test[BODY] = self.test[BODY].apply(lambda x: re.sub(ASCII_REGEX, \"\", x))\n",
    "            self.test[BODY] = self.test[BODY].apply(lambda x: ud.normalize(\"NFD\", x))\n",
    "\n",
    "        logger.debug(\"Replacing fixed part of REPO URl column...\")\n",
    "        self.dedup_train[REPO] = self.dedup_train[REPO].apply(lambda x: x.replace(\"https://api.github.com/repos/\", \"\"))\n",
    "        \n",
    "        if(self.evaluation_mode): self.eval[REPO] = self.eval[REPO].apply(lambda x: x.replace(\"https://api.github.com/repos/\", \"\"))\n",
    "\n",
    "        if(self.test_mode): self.test[REPO] = self.test[REPO].apply(lambda x: x.replace(\"https://api.github.com/repos/\", \"\"))\n",
    "\n",
    "        logger.debug(\"Replacing white spaces...\")\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].apply(lambda x:\" \".join(x.split()))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x:\" \".join(x.split()))\n",
    "\n",
    "        if(self.evaluation_mode):\n",
    "            self.eval[TITLE] = self.eval[TITLE].apply(lambda x:\" \".join(x.split()))\n",
    "            self.eval[BODY] = self.eval[BODY].apply(lambda x:\" \".join(x.split()))\n",
    "        \n",
    "        if(self.test_mode):\n",
    "            self.test[TITLE] = self.test[TITLE].apply(lambda x:\" \".join(x.split()))\n",
    "            self.test[BODY] = self.test[BODY].apply(lambda x:\" \".join(x.split()))\n",
    "\n",
    "    def truncate_columns(self):\n",
    "        \"\"\"\n",
    "        Truncates the \"TITLE\" and \"BODY\" fields to a maximum length.\n",
    "        - Uses MAX_TITLE_LENGTH and MAX_BODY_LENGTH constants to truncate text fields.\n",
    "        \"\"\"\n",
    "        self.dedup_train[TITLE] = self.dedup_train[TITLE].apply(lambda x: \" \".join(x.split(maxsplit=MAX_TITLE_LENGTH)[:MAX_TITLE_LENGTH]))\n",
    "        self.dedup_train[BODY] = self.dedup_train[BODY].apply(lambda x: \" \".join(x.split(maxsplit=MAX_BODY_LENGTH)[:MAX_BODY_LENGTH]))\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            self.eval[TITLE] = self.eval[TITLE].apply(lambda x: \" \".join(x.split(maxsplit=MAX_TITLE_LENGTH)[:MAX_TITLE_LENGTH]))\n",
    "            self.eval[BODY] = self.eval[BODY].apply(lambda x: \" \".join(x.split(maxsplit=MAX_BODY_LENGTH)[:MAX_BODY_LENGTH]))\n",
    "\n",
    "        if self.test_mode:\n",
    "            self.test[TITLE] = self.test[TITLE].apply(lambda x: \" \".join(x.split(maxsplit=MAX_TITLE_LENGTH)[:MAX_TITLE_LENGTH]))\n",
    "            self.test[BODY] = self.test[BODY].apply(lambda x: \" \".join(x.split(maxsplit=MAX_BODY_LENGTH)[:MAX_BODY_LENGTH]))\n",
    "\n",
    "    def extract_label_column(self): # TODO: function to reverse labels back to original ones/to the ones wanted for predictions!\n",
    "        \"\"\"\n",
    "        Converts the label column into categorical data and extracts label codes.\n",
    "        - Uses the \"label\" column and generates categorical codes for the model.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(\"extract_label_column - START - value_counts() for dataframes:\")\n",
    "        if(self.dedup_train is not None):\n",
    "            logger.debug(self.dedup_train[LABEL].value_counts())\n",
    "        if(self.eval is not None):\n",
    "            logger.debug(self.eval[LABEL].value_counts())\n",
    "        if(self.test is not None):\n",
    "            logger.debug(self.test[LABEL].value_counts())\n",
    "        \n",
    "        self.dedup_train[LABEL] = pd.Categorical(self.dedup_train[LABEL])\n",
    "        # Save original labels so the predictions of the model can be reverted later on\n",
    "        self.label_mapping_dict = dict(enumerate(self.dedup_train[LABEL].cat.categories))\n",
    "        logger.debug(f\"self.label_mapping_dict = {self.label_mapping_dict}\")\n",
    "        if self.evaluation_mode:\n",
    "            self.eval[LABEL] = pd.Categorical(self.eval[LABEL])\n",
    "        if self.test_mode:\n",
    "            self.test[LABEL] = pd.Categorical(self.test[LABEL])\n",
    "\n",
    "        # for label in self.label_names.keys():\n",
    "        #     logger.debug(self.dedup_train[self.dedup_train[LABEL] == label].iloc[0])\n",
    "        self.dedup_train[LABEL_COL] = self.dedup_train[LABEL].cat.codes\n",
    "        # for label in range(len(self.label_names.keys())):\n",
    "        #     logger.debug(self.dedup_train[self.dedup_train[LABEL_COL] == label].iloc[0])\n",
    "        logger.debug(self.dedup_train[LABEL_COL].value_counts())\n",
    "        \n",
    "        if self.evaluation_mode:\n",
    "            # for label in self.label_names.keys():\n",
    "            #     logger.debug(self.eval[self.eval[LABEL] == label].iloc[0])\n",
    "            self.eval[LABEL_COL] = self.eval[LABEL].cat.codes\n",
    "            # for label in range(len(self.label_names.keys())):\n",
    "            #     logger.debug(self.eval[self.eval[LABEL_COL] == label].iloc[0])\n",
    "            logger.debug(self.eval[LABEL_COL].value_counts())\n",
    "        \n",
    "        if self.test_mode:\n",
    "            # for label in self.label_names.keys():\n",
    "            #     logger.debug(self.test[self.test[LABEL] == label].iloc[0])\n",
    "            self.test[LABEL_COL] = self.test[LABEL].cat.codes\n",
    "            # for label in range(len(self.label_names.keys())):\n",
    "            #     logger.debug(self.test[self.test[LABEL_COL] == label].iloc[0])\n",
    "            logger.debug(self.test[LABEL_COL].value_counts())\n",
    "\n",
    "        logger.debug(\"extract_label_column - END - value_counts() for dataframes:\")\n",
    "        if(self.dedup_train is not None):\n",
    "            logger.debug(self.dedup_train[LABEL].value_counts())\n",
    "        if(self.eval is not None):\n",
    "            logger.debug(self.eval[LABEL].value_counts())\n",
    "        if(self.test is not None):\n",
    "            logger.debug(self.test[LABEL].value_counts())\n",
    "\n",
    "    def prepare_columns_for_model(self):\n",
    "        \"\"\"\n",
    "        Prepares the final \"text\" and \"label\" columns for model training.\n",
    "        - Concatenates different text features into a single \"text_col\" for model input.\n",
    "        \"\"\"\n",
    "        self.dedup_train[TEXT_COL] = \"time \" + self.dedup_train[TIME] + \" author \" + self.dedup_train[AUTHOR] +\" repo \" + self.dedup_train[REPO] + \" title \" + self.dedup_train[TITLE] + \" body \" + self.dedup_train[BODY]\n",
    "        self.dedup_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if self.evaluation_mode:\n",
    "            self.eval[TEXT_COL] = \"time \" + self.eval[TIME] + \" author \" + self.eval[AUTHOR] +\" repo \" + self.eval[REPO] + \" title \" + self.eval[TITLE] + \" body \" + self.eval[BODY]\n",
    "            self.eval.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if self.test_mode:\n",
    "            self.test[TEXT_COL] = \"time \" + self.test[TIME] + \" author \" + self.test[AUTHOR] +\" repo \" + self.test[REPO] + \" title \" + self.test[TITLE] + \" body \" + self.test[BODY]\n",
    "            self.test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        logger.debug(f\"Number of train issues: {self.train.shape}\")\n",
    "        if self.evaluation_mode:\n",
    "            logger.debug(f\"Number of eval issues: {self.eval.shape}\")\n",
    "        if self.test_mode:\n",
    "            logger.debug(f\"Number of test issues: {self.test.shape}\")\n",
    "\n",
    "    def save_data(self):\n",
    "        \"\"\"\n",
    "        Saves the processed data to CSV files for training, evaluation, and testing.\n",
    "        - File paths are constructed using PROJECT_FOLDER and DATA_FOLDER constants.\n",
    "        \"\"\"       \n",
    "        self.dedup_train[[TEXT_COL, LABEL_COL]].to_csv(self.train_csv, index=False)\n",
    "        \n",
    "        if self.evaluation_mode:\n",
    "            self.eval[[TEXT_COL, LABEL_COL]].to_csv(self.eval_csv, index=False)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            self.test[[TEXT_COL, LABEL_COL]].to_csv(self.test_csv, index=False)\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Executes the full data processing pipeline:\n",
    "        - Create DataFrame, split into train/eval/test sets, deduplicate data.\n",
    "        - Normalize text, clean extraneous information, and prepare columns for model input.\n",
    "        \"\"\"\n",
    "        logger.debug(\"create df\")\n",
    "        self.create_df()\n",
    "        logger.debug(\"filter df\")\n",
    "        self.filter_df()\n",
    "        logger.debug(\"split data\")\n",
    "        self.split_data()\n",
    "        logger.debug(\"deduplicate data\")\n",
    "        self.deduplicate_data()\n",
    "        logger.debug(\"normalize text\")\n",
    "        self.normalize_text()\n",
    "        logger.debug(\"remove extra info from texts\")\n",
    "        self.remove_extra_info()\n",
    "        logger.debug(\"truncate columns\")\n",
    "        self.truncate_columns()\n",
    "        logger.debug(\"extract label column\")\n",
    "        self.extract_label_column()\n",
    "        logger.debug(\"prepare columns for model\")\n",
    "        self.prepare_columns_for_model()\n",
    "\n",
    "        global EXPORT_ML_DATA_CSVS\n",
    "        if EXPORT_ML_DATA_CSVS:\n",
    "            self.save_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the processed training, evaluation, and test data from CSV files.\n",
    "        - File paths are constructed using PROJECT_FOLDER and DATA_FOLDER constants.\n",
    "        \"\"\"\n",
    "        if os.file.exists(self.train_csv):\n",
    "            self.dedup_train = pd.read_csv(self.train_csv)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Training data file not found: {self.train_csv}\")\n",
    "        \n",
    "        if self.evaluation_mode:\n",
    "            if os.file.exists(self.eval_csv):\n",
    "                self.eval = pd.read_csv(self.eval_csv)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Evaluation data file not found: {self.eval_csv}\")\n",
    "                \n",
    "        if self.test_mode:\n",
    "            if os.file.exists(self.test_csv):\n",
    "                self.test = pd.read_csv(self.test_csv)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Test data file not found: {self.test_csv}\")\n",
    "\n",
    "    def get_train_data(self):\n",
    "        \"\"\"\n",
    "        Returns the training data DataFrame.\n",
    "        \"\"\"\n",
    "        return self.dedup_train\n",
    "    \n",
    "    def get_eval_data(self):\n",
    "        \"\"\"\n",
    "        Returns the evaluation data DataFrame.\n",
    "        \"\"\"\n",
    "        return self.eval\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \"\"\"\n",
    "        Returns the test data DataFrame.\n",
    "        \"\"\"\n",
    "        return self.test\n",
    "\n",
    "    def get_label_mapping_dict(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary that contains the label mapping.\n",
    "        \"\"\"\n",
    "        return self.label_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_NAME = \"vaadin\"\n",
    "# REPO_NAME = \"flow\"\n",
    "\n",
    "# USER_NAME = \"grafana\"\n",
    "# REPO_NAME = \"grafana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# repoHandler = RepoDataHandler(user_name=USER_NAME, repository_name=REPO_NAME, token=GITHUB_PAT)\n",
    "# repoHandler.load_repo_data_config()\n",
    "# repoHandler.load_data()\n",
    "# if FETCH_NEW_ISSUES:\n",
    "#     repoHandler.fetch_issues()\n",
    "#     repoHandler.save_data(partial=True)\n",
    "# issue_data = repoHandler.get_issues()\n",
    "\n",
    "# repoHandler.load_label_config()\n",
    "# label_names = repoHandler.get_label_dict()\n",
    "# logger.debug(label_names)\n",
    "\n",
    "# dataProcessor = DataProcessor(data=issue_data, label_names=label_names, user_name=USER_NAME, repository_name=REPO_NAME)\n",
    "# dataProcessor.process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# # Number of labels\n",
    "# num_labels = 3  # Adjust as per your use case\n",
    "\n",
    "# # Load model configuration with correct number of labels\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
    "\n",
    "# # Load the state dictionary from your .bin file\n",
    "# state_dict = torch.load(PRETRAINED_MODEL_FILE_NAME, map_location=device)\n",
    "\n",
    "# # Load the state dict into the model\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # Move model to the device (CPU or GPU)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueLabelPredictor:\n",
    "    def __init__(self, \n",
    "                 model_name=\"roberta\", \n",
    "                 model_version=\"roberta-base\", \n",
    "                 learning_rate=3e-5, \n",
    "                 epochs=4, \n",
    "                 # batch_size=100,\n",
    "                 batch_size=50,\n",
    "                 max_seq_length=200, \n",
    "                 num_labels=3, # len(label_names)\n",
    "                 output_dir=os.path.join(RESULTS_FOLDER, FINETUNED_MODEL_FOLDER), \n",
    "                 use_cuda=torch.cuda.is_available(),\n",
    "                 use_fine_tuned_model=False,\n",
    "                 pretrained_model_path=os.path.join(PROJECT_FOLDER, MODEL_FOLDER, PRETRAINED_MODEL_FILE_NAME),\n",
    "                 finetuned_model_path=os.path.join(PROJECT_FOLDER, MODEL_FOLDER, FINETUNED_MODEL_FOLDER, FINETUNED_MODEL_FILE_NAME)\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initializes the IssueLabelPredictor for handling ML tasks.\n",
    "\n",
    "        :param model_name: Name of the model architecture.\n",
    "        :param model_version: Specific version of the model.\n",
    "        :param learning_rate: Learning rate for training.\n",
    "        :param epochs: Number of training epochs.\n",
    "        :param batch_size: Batch size for training and evaluation.\n",
    "        :param max_seq_length: Maximum sequence length for inputs.\n",
    "        :param num_labels: Number of target labels/classes.\n",
    "        :param output_dir: Directory to save or load the model.\n",
    "        :param use_cuda: Whether to use GPU acceleration.\n",
    "        :param use_fine_tuned_model: Whether to load a fine-tuned model or a pretrained one.\n",
    "        :param pretrained_model_path: Path to the pretrained model file.\n",
    "        :param finetuned_model_path: Path to the fine-tuned model file.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_labels = num_labels\n",
    "        self.output_dir = output_dir\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "        self.use_fine_tuned_model = use_fine_tuned_model\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.finetuned_model_path = finetuned_model_path\n",
    "        # Initialize model and tokenizer\n",
    "        self._init_model_and_tokenizer()\n",
    "    \n",
    "    def _init_model_and_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Initializes the model and tokenizer.\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_version)\n",
    "        \n",
    "        # Load the fine-tuned model if specified\n",
    "        if self.use_fine_tuned_model and self.finetuned_model_path and os.path.exists(self.finetuned_model_path):\n",
    "            self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                self.finetuned_model_path\n",
    "            )\n",
    "            logger.info(f\"Loaded fine-tuned model from {self.finetuned_model_path}\")\n",
    "        \n",
    "        elif self.pretrained_model_path and os.path.exists(self.pretrained_model_path):\n",
    "            if os.path.isdir(self.pretrained_model_path):\n",
    "                self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                    self.pretrained_model_path,\n",
    "                    num_labels=self.num_labels\n",
    "                )\n",
    "                self.tokenizer = RobertaTokenizer.from_pretrained(self.pretrained_model_path)\n",
    "                logger.info(f\"Loaded pretrained model from directory {self.pretrained_model_path}\")\n",
    "            else:\n",
    "                # Load model from pytorch_model.bin file\n",
    "                self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                    self.model_version, \n",
    "                    num_labels=self.num_labels\n",
    "                )\n",
    "                state_dict = torch.load(self.pretrained_model_path, map_location=self.device)\n",
    "                \n",
    "                # Load the state dict with strict=False to ignore unexpected keys\n",
    "                self.model.load_state_dict(state_dict, strict=False)\n",
    "                logger.info(f\"Loaded pretrained model from file {self.pretrained_model_path}\")\n",
    "        else:\n",
    "            self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                self.model_version, \n",
    "                num_labels=self.num_labels\n",
    "            )\n",
    "            logger.info(f\"Initialized new model from {self.model_version}\")\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()                                                                                                                                                                                                                                                                                       \n",
    "\n",
    "    def train(self, train_df, eval_df=None):\n",
    "        \"\"\"\n",
    "        Trains the model using the provided training DataFrame.\n",
    "        \"\"\"\n",
    "        from transformers import Trainer, TrainingArguments\n",
    "\n",
    "        # Prepare datasets\n",
    "        train_dataset = self._prepare_dataset(train_df)\n",
    "        eval_dataset = self._prepare_dataset(eval_df) if eval_df is not None else None\n",
    "\n",
    "        # Determine the strategy based on whether eval_df is provided\n",
    "        strategy = \"epoch\" if eval_df is not None else \"no\"\n",
    "\n",
    "        # Set up training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size,\n",
    "            learning_rate=self.learning_rate,\n",
    "            evaluation_strategy=strategy,\n",
    "            save_strategy=strategy,\n",
    "            logging_dir=os.path.join(self.output_dir, \"logs\"),\n",
    "            logging_steps=50,\n",
    "            load_best_model_at_end=True if eval_df is not None else False,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            use_cpu=not self.use_cuda,\n",
    "            overwrite_output_dir=True,\n",
    "            report_to=\"none\"  # Disable wandb and other logging integrations\n",
    "        )\n",
    "\n",
    "        # Define compute_metrics function\n",
    "        # def compute_metrics(pred):\n",
    "        #     labels = pred.label_ids\n",
    "        #     preds = pred.predictions.argmax(-1)\n",
    "        #     report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "        #     return {\n",
    "        #         \"accuracy\": report[\"accuracy\"],\n",
    "        #         \"f1_micro\": report[\"micro avg\"][\"f1-score\"]\n",
    "        #     }\n",
    "\n",
    "        def compute_metrics(pred):\n",
    "            labels = pred.label_ids\n",
    "            preds = pred.predictions.argmax(-1)\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "            return {\n",
    "                \"accuracy\": acc,\n",
    "                \"f1_micro\": f1_micro\n",
    "            }\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics if eval_df is not None else None\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model and tokenizer\n",
    "        # self.model.save_pretrained(self.output_dir)\n",
    "        # self.tokenizer.save_pretrained(self.output_dir)\n",
    "        self.model.save_pretrained(os.path.dirname(self.finetuned_model_path))\n",
    "        self.tokenizer.save_pretrained(os.path.dirname(self.finetuned_model_path))\n",
    "        \n",
    "\n",
    "    def _prepare_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Prepares a list of dictionaries from a DataFrame.\n",
    "\n",
    "        :param df: DataFrame containing 'text' and 'labels' columns.\n",
    "        :return: List of dictionaries ready for Trainer.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            return None\n",
    "\n",
    "        texts = df[TEXT_COL].tolist()\n",
    "        labels = df[LABEL_COL].tolist()\n",
    "\n",
    "        # Tokenize the texts\n",
    "        encodings = self.tokenizer(\n",
    "            texts, \n",
    "            max_length=self.max_seq_length, \n",
    "            truncation=True, \n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        # Convert to list of dictionaries\n",
    "        dataset = []\n",
    "        for i in range(len(texts)):\n",
    "            item = {key: torch.tensor(val[i]) for key, val in encodings.items()}\n",
    "            item['labels'] = torch.tensor(labels[i])\n",
    "            dataset.append(item)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"\n",
    "        Predicts labels for the provided DataFrame.\n",
    "\n",
    "        :param df: DataFrame containing a \"text\" column.\n",
    "        :return: DataFrame with an additional \"predicted_label\" column.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        texts = df[TEXT_COL].tolist()\n",
    "\n",
    "        # Tokenize the texts\n",
    "        inputs = self.tokenizer(\n",
    "            texts, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=self.max_seq_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Move tensors to device\n",
    "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids, attention_mask = batch\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        df = df.copy()\n",
    "        df[PREDICTED_LABEL] = all_predictions\n",
    "        # logger.debug(f\"Predictions: {df.head()}\")\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, df):\n",
    "        \"\"\"\n",
    "        Evaluates the model on the provided DataFrame.\n",
    "\n",
    "        :param df: DataFrame containing \"text\" and \"labels\" columns.\n",
    "        :return: Classification report as a string.\n",
    "        \"\"\"\n",
    "        df_with_predictions = self.predict(df)\n",
    "        y_true = df_with_predictions[LABEL_COL]\n",
    "        y_pred = df_with_predictions[PREDICTED_LABEL]\n",
    "\n",
    "        report = classification_report(y_true, y_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "        # Prepare detailed report\n",
    "        report_with_class_accuracy = f\"Classification Report:\\n{report}\\nPer-Class Accuracy:\\n\"\n",
    "        for i, acc in enumerate(per_class_accuracy):\n",
    "            report_with_class_accuracy += f\"Class {i}: {acc:.2f}\\n\"\n",
    "\n",
    "        return report_with_class_accuracy\n",
    "\n",
    "    def analyze_predictions(self, df):\n",
    "        \"\"\"\n",
    "        Analyzes predictions for unlabeled data.\n",
    "\n",
    "        :param df: DataFrame containing \"text\" and \"predicted_label\" columns.\n",
    "        \"\"\"\n",
    "        class_distribution = df[PREDICTED_LABEL].value_counts()\n",
    "\n",
    "        # Visualizing class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        class_distribution.plot(kind=\"bar\")\n",
    "        plt.title(\"Class Distribution of Predicted Labels\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "        # # Sampling data for manual evaluation\n",
    "        # sample_size = 10  # Number of samples to pick per class\n",
    "        # sampled_data = df.groupby(\"predicted_label\").apply(\n",
    "        #     lambda x: x.sample(min(len(x), sample_size))\n",
    "        # )\n",
    "\n",
    "        # logger.debug(\"\\nSampled Data for Manual Evaluation:\\n\", sampled_data[[\"predicted_label\", \"text\"]])\n",
    "\n",
    "    def save_predictions(self, df, output_file_path=os.path.join(PROJECT_FOLDER, RESULTS_FOLDER, \"predictions.csv\")):\n",
    "        \"\"\"\n",
    "        Saves the predictions to a CSV file.\n",
    "\n",
    "        :param df: DataFrame containing predictions.\n",
    "        :param output_file_path: File path to save the predictions.\n",
    "        \"\"\"\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "        logger.debug(f\"Predictions saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER_NAME = \"vaadin\"\n",
    "# REPO_NAME = \"flow\"\n",
    "\n",
    "# USER_NAME = \"grafana\"\n",
    "# REPO_NAME = \"grafana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repoHandler = RepoDataHandler(user_name=USER_NAME, repository_name=REPO_NAME, token=GITHUB_PAT)\n",
    "# repoHandler.load_repo_data_config(config_file=REPO_DATA_CONFIG_FILE_NAME) # os.path.join(PROJECT_FOLDER, CONFIG_FOLDER, REPO_DATA_CONFIG_FILE_NAME)\n",
    "repoHandler.load_repo_data_config()\n",
    "repoHandler.load_data()\n",
    "if FETCH_NEW_ISSUES:\n",
    "    repoHandler.fetch_issues()\n",
    "    repoHandler.save_data(partial=True)\n",
    "issue_data = repoHandler.get_issues()\n",
    "logger.debug(f\"Length of issue data: {len(issue_data)}\")\n",
    "\n",
    "repoHandler.load_label_config()\n",
    "label_names = repoHandler.get_label_dict()\n",
    "logger.debug(label_names)\n",
    "\n",
    "dataProcessor = DataProcessor(data=issue_data, label_names=label_names, user_name=USER_NAME, repository_name=REPO_NAME)\n",
    "dataProcessor.process_data()\n",
    "\n",
    "train_df = eval_df = test_df = None\n",
    "\n",
    "train_df = dataProcessor.get_train_data()\n",
    "if(train_df is not None):\n",
    "    logger.debug(f\"Shape of training data frame: {train_df.shape}\")\n",
    "eval_df = dataProcessor.get_eval_data()\n",
    "if(eval_df is not None):\n",
    "    logger.debug(f\"Shape of evaluation data frame: {eval_df.shape}\")\n",
    "test_df = dataProcessor.get_test_data()\n",
    "if(test_df is not None):\n",
    "    logger.debug(f\"Shape of test data frame: {test_df.shape}\")\n",
    "label_mapping_dict = dataProcessor.get_label_mapping_dict()\n",
    "logger.debug(f\"Label mapping dict: {label_mapping_dict}\")\n",
    "\n",
    "eval_df_copy = dataProcessor.get_eval_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = False\n",
    "\n",
    "if not BASELINE:\n",
    "    # Initialize the predictor\n",
    "    predictor = IssueLabelPredictor()\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    predictor.train(train_df, eval_df=eval_df)\n",
    "\n",
    "\n",
    "else:\n",
    "    baseline_predictor = IssueLabelPredictor()\n",
    "    baseline_predictor.predict(eval_df_copy)\n",
    "    \n",
    "    if eval_df_copy is not None:\n",
    "        report = baseline_predictor.evaluate(eval_df_copy)\n",
    "        try:\n",
    "            eval_df_copy[LABEL_COL] = eval_df_copy[LABEL_COL].map(label_mapping_dict) # TODO: TEST\n",
    "        except Exception as e:\n",
    "            logger.error(f\"When trying to map labels for evaluation DataFrame Error {e} occured\")\n",
    "        logger.debug(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report = None\n",
    "\n",
    "# Evaluate the model\n",
    "if len(eval_df) > 0:\n",
    "    report = predictor.evaluate(eval_df)\n",
    "    try:\n",
    "        eval_df[LABEL_COL] = eval_df[LABEL_COL].map(label_mapping_dict) # TODO: TEST\n",
    "    except Exception as e:\n",
    "        logger.error(f\"When trying to map labels for evaluation DataFrame Error {e} occured\")\n",
    "    logger.debug(report)\n",
    "\n",
    "# Make predictions on test data\n",
    "if len(test_df) > 0:\n",
    "    test_df = predictor.predict(test_df)\n",
    "    try:\n",
    "        test_df[PREDICTED_LABEL] = test_df[PREDICTED_LABEL].map(label_mapping_dict)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"When trying to map labels for test DataFrame Error {e} occured\")\n",
    "\n",
    "    if EXPORT_PREDICTIONS:\n",
    "        predictor.save_predictions(test_df)\n",
    "\n",
    "    # TODO: delete START\n",
    "    repoHandler.load_results_dataframe(df=test_df)\n",
    "    try:\n",
    "        repoHandler.save_predictions_to_issue_data()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Updating issue data failed: {e}\")\n",
    "    # TODO: delete END\n",
    "\n",
    "    # # TODO: uncomment START\n",
    "    # if UPDATE_ISSUE_DATA or UPDATE_GITHUB_LABELS:\n",
    "    #     test_df[PREDICTED_LABEL] = test_df[PREDICTED_LABEL].map(repoHandler.get_prediction_labels()) # TODO: TEST\n",
    "    #     repoHandler.load_results_dataframe(df=test_df)\n",
    "    #     if UPDATE_ISSUE_DATA and UPDATE_GITHUB_LABELS:\n",
    "    #         try:\n",
    "    #             repoHandler.save_predictions_to_issue_data_and_update_issues()\n",
    "    #         except Exception as e:\n",
    "    #             logger.error(f\"Updating issues failed: {e}\")\n",
    "    #     if UPDATE_ISSUE_DATA:\n",
    "    #         try:\n",
    "    #             repoHandler.save_predictions_to_issue_data()\n",
    "    #         except Exception as e:\n",
    "    #             logger.error(f\"Updating issue data failed: {e}\")\n",
    "    # # TODO: uncomment END\n",
    "\n",
    "    # # If test data is unlabeled\n",
    "    # predictor.analyze_predictions(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[PREDICTED_LABEL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_df) > 0 and EXPORT_PREIDCTIONS: # replace True with check for setting from config\n",
    "    test_df_export = test_df[[NUMBER, PREDICTED_LABEL]]\n",
    "    test_df_export.to_csv(os.path.join(PROJECT_FOLDER, RESULTS_FOLDER, f\"predictions.csv\"), index=False)\n",
    "    # test_df_export.to_csv(os.path.join(PROJECT_FOLDER, RESULTS_FOLDER, f\"{USER_NAME}_{REPO_NAME}_predictions.csv\"), index=False)\n",
    "    # test_df_export.to_csv(os.path.join(PROJECT_FOLDER, RESULTS_FOLDER, f\"{USER_NAME}_{REPO_NAME}_predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_ISSUE_DATA and len(test_df) > 0:\n",
    "    logger.debug(\"Value counts for results dataframe:\")\n",
    "    logger.debug(test_df[PREDICTED_LABEL].value_counts())\n",
    "    repoHandler.save_predictions_to_issue_data()\n",
    "\n",
    "if UPDATE_GITHUB_LABELS: repoHandler.update_issues(df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_USER_NAME = \"test\"\n",
    "# TEST_REPO_NAME = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize your RepoDataHandler with your test repository\n",
    "# handler = RepoDataHandler(\n",
    "#     user_name=TEST_USER_NAME,\n",
    "#     repository_name=TEST_REPO_NAME,\n",
    "#     token=GITHUB_PAT\n",
    "# )\n",
    "\n",
    "# # Ensure GitHub API setup\n",
    "# handler.setup_github_api()\n",
    "\n",
    "# if UPDATE_ISSUE_DATA and len(test_df) > 0:\n",
    "#     logger.debug(\"Value counts for results dataframe:\")\n",
    "#     logger.debug(test_df[PREDICTED_LABEL].value_counts())\n",
    "#     repoHandler.save_predictions_to_issue_data()\n",
    "\n",
    "# if UPDATE_GITHUB_LABELS: repoHandler.update_issues(df=test_df) # TODO: test by removing some labels for issues of my repo and then test the whole setup, including adding predicted labels to github repo\n",
    "        \n",
    "#     # csv_path = \"../results/predictions.csv\"  # Path to the CSV file with results\n",
    "#     # handler.load_results_dataframe(csv_path=csv_path)  # Load predictions into handler\n",
    "\n",
    "\n",
    "# def sync_issues_with_test_repo(handler):\n",
    "#     counter = 0\n",
    "#     for _, row in handler.results_df.iterrows():\n",
    "#         if counter >= 10: return\n",
    "#         try:\n",
    "#             # Check if the issue already exists by title\n",
    "#             existing_issues = list(handler.repo.get_issues(state=\"all\"))\n",
    "#             matched_issue = next((issue for issue in existing_issues if issue.title == row[\"title\"]), None)\n",
    "\n",
    "#             if matched_issue:\n",
    "#                 print(f\"Issue {matched_issue.number} already exists in the test repo. Updating labels.\")\n",
    "#                 handler.update_issue(issue_number=matched_issue.number, predicted_label=row[PREDICTED_LABEL])\n",
    "#             else:\n",
    "#                 # Create a new issue if not found\n",
    "#                 new_issue = handler.repo.create_issue(\n",
    "#                     title=row[\"title\"],\n",
    "#                     body=row[\"description\"]\n",
    "#                 )\n",
    "#                 print(f\"Created new issue {new_issue.number} from original issue {row['number']}\")\n",
    "#                 handler.update_issue(issue_number=new_issue.number, predicted_label=row[PREDICTED_LABEL])\n",
    "#                 counter += 1\n",
    "            \n",
    "#             print(f\"Updated issue {row['number']} in test repo.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error handling issue {row['number']}: {e}\")\n",
    "\n",
    "# # sync_issues_with_test_repo(handler)\n",
    "\n",
    "# # TODO: update json file with new pushed labels, so next time only new unlabeled issues will have predictions done for (also introduce parameter that saves if the label was hand-made or a prediction of this model and also implement an option to only use hand-made labels for fine-tuning and ignore the issues with predicted labels completely)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SmartDelta]",
   "language": "python",
   "name": "conda-env-SmartDelta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
